{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "e1d4880ad13fa2099fc93eba0cb791232af4f1a31a1c632661aaef6a29f2ead6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Develop lstm model for time series prediction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# interactive figures\r\n",
    "%matplotlib widget \r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "# model evalution metrics\r\n",
    "from sklearn.metrics import mean_absolute_error\r\n",
    "from sklearn.metrics import mean_squared_error\r\n",
    "\r\n",
    "# data preprocessing\r\n",
    "from sklearn.preprocessing import normalize\r\n",
    "\r\n",
    "# predictive models\r\n",
    "from sklearn.linear_model import LinearRegression\r\n",
    "from sklearn.svm import LinearSVR\r\n",
    "from sklearn.neural_network import MLPRegressor\r\n",
    "\r\n",
    "# keras stuff\r\n",
    "import math\r\n",
    "from keras.models import Sequential\r\n",
    "from keras.layers import Dense\r\n",
    "from keras.layers import LSTM\r\n",
    "from sklearn.preprocessing import MinMaxScaler\r\n",
    "from sklearn.metrics import mean_squared_error"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class time_series_prediction():\r\n",
    "\r\n",
    "    def __init__(self,dates,one_d_time_series,lag_window_length,n_ahead_prediction):\r\n",
    "\r\n",
    "        # raw input data + settings for time series -> supervised learning ML problem\r\n",
    "        self.one_d_time_series = np.array(one_d_time_series)      # time series array, to array ensure index works as expected for class methods\r\n",
    "        self.time_series_dates = np.array(dates)                  # time stamp / date for each data point\r\n",
    "        self.lag_window_length = lag_window_length                # length of lag window\r\n",
    "        self.n_ahead_prediction = n_ahead_prediction              # time ahead to predict\r\n",
    "\r\n",
    "        # transfromed data: set after calling .sliding_window_1()\r\n",
    "        self.input_data = None\r\n",
    "        self.target_data = None\r\n",
    "\r\n",
    "        # testing and training data: set after calling .train_test_split()\r\n",
    "        self.training_split = None\r\n",
    "        self.X_test = None\r\n",
    "        self.X_train = None\r\n",
    "        self.y_test = None\r\n",
    "        self.y_train = None\r\n",
    "\r\n",
    "        # predictions from various models - set after calling each models training\r\n",
    "        self.linear_reg_predictions = None\r\n",
    "        self.svm_predictions = None\r\n",
    "        self.neural_net_predictions = None\r\n",
    "        self.naive_predictions = None\r\n",
    "\r\n",
    "        # cumprod results from predictions - set after calling .vis_results_time_series()\r\n",
    "        self.real_vals_cumprod = None\r\n",
    "        self.linear_reg_predictions_cumprod = None\r\n",
    "        self.svm_predictions_cumprod = None\r\n",
    "        self.neural_net_predictions_cumprod = None\r\n",
    "        self.lstm_predictions = None\r\n",
    "\r\n",
    "# ****************************************************************************************************************\r\n",
    "    # data wrangling\r\n",
    "# ****************************************************************************************************************\r\n",
    "\r\n",
    "    # method to transfroms 1-D time series to supervised ML problem: one step ahead forecasting   \r\n",
    "    def sliding_window_1(self,verbose):\r\n",
    "        # initialize input array\r\n",
    "        num_rows = len(self.one_d_time_series) - self.lag_window_length\r\n",
    "        array = np.zeros((num_rows, self.lag_window_length + 1))\r\n",
    "        \r\n",
    "        # loop through data and populate array\r\n",
    "        for i in range(num_rows):\r\n",
    "            # input features\r\n",
    "            array[i,0:self.lag_window_length+1] = self.one_d_time_series[i:i+self.lag_window_length+1]\r\n",
    "            # target feature/s\r\n",
    "            array[i,-1] = self.one_d_time_series[i+self.lag_window_length]\r\n",
    "            \r\n",
    "            if verbose == 1:\r\n",
    "                # show pattern\r\n",
    "                print(array[i,0:self.lag_window_length],' : ',array[i,self.lag_window_length])\r\n",
    "\r\n",
    "        # save results as a class attribute\r\n",
    "        self.input_data = array[:,0:self.lag_window_length]\r\n",
    "        self.target_data = array[:,self.lag_window_length]\r\n",
    "\r\n",
    "    # method to perform a training and testing split for dataset with only a single column of target variables\r\n",
    "    def train_test_split(self,split):\r\n",
    "        self.training_split = split\r\n",
    "        self.X_train = self.input_data[0:split,:]\r\n",
    "        self.X_test = self.input_data[split:,:]\r\n",
    "        self.y_train = self.target_data[0:split]\r\n",
    "        self.y_test = self.target_data[split:]\r\n",
    "\r\n",
    "    # method to plot testing and training split of data\r\n",
    "    def test_train_plot(self):\r\n",
    "        fig, ax = plt.subplots(figsize=(10,5))\r\n",
    "        ax.plot(self.time_series_dates[0:self.training_split] ,self.one_d_time_series[0:self.training_split],'k-',label='Training data') # replace returns with sp_500 for other data plotting\r\n",
    "        ax.plot(self.time_series_dates[self.training_split:] ,self.one_d_time_series[self.training_split:],'r-',label='Testing data')\r\n",
    "        ax.plot(self.time_series_dates[self.training_split+self.lag_window_length:] ,self.y_test,'o',label='Windowed testing data') # important to match time by start 5 (length of time window) after where segmented our testing and training data\r\n",
    "        plt.legend(loc=0) \r\n",
    "        ax.set_xticks([self.time_series_dates[x] for x in range(0,len(self.time_series_dates),150)])\r\n",
    "        ax.tick_params(rotation=30) \r\n",
    "        plt.tight_layout()\r\n",
    "\r\n",
    "# ****************************************************************************************************************\r\n",
    "    # predictive models\r\n",
    "# ****************************************************************************************************************\r\n",
    "\r\n",
    "    def linear_regression(self):\r\n",
    "        print('Training multivariate linear regression:')\r\n",
    "        # train model\r\n",
    "        reg_model = LinearRegression().fit(self.X_train,self.y_train)\r\n",
    "        print('\\nLinear regression coefficients: \\n',reg_model.coef_)\r\n",
    "\r\n",
    "        # test model\r\n",
    "        predictions = reg_model.predict(self.X_test)\r\n",
    "\r\n",
    "        # evaluate: use sklearn metric methods to calc rmse and mae\r\n",
    "        mse = mean_squared_error(self.y_test,predictions)\r\n",
    "        mae = mean_absolute_error(self.y_test,predictions)\r\n",
    "\r\n",
    "        print('RMSE: ',np.sqrt(mse))\r\n",
    "        print('MAE: ',mae)\r\n",
    "\r\n",
    "        # save predictions\r\n",
    "        self.linear_reg_predictions = predictions\r\n",
    "\r\n",
    "    def support_vector_machine(self):\r\n",
    "        print('\\nTraining support vector machine:')\r\n",
    "        # train model\r\n",
    "        svm_regres = LinearSVR(max_iter=1000,C=0.5).fit(self.X_train,self.y_train)\r\n",
    "\r\n",
    "        # predict\r\n",
    "        svm_predictions = svm_regres.predict(self.X_test)\r\n",
    "\r\n",
    "        # evaluate\r\n",
    "        mse = mean_squared_error(self.y_test,svm_predictions[:])\r\n",
    "        mae = mean_absolute_error(self.y_test,svm_predictions[:])\r\n",
    "\r\n",
    "        print('RMSE: ',np.sqrt(mse))\r\n",
    "        print('MAE: ',mae)\r\n",
    "\r\n",
    "        # save predictions\r\n",
    "        self.svm_predictions = svm_predictions\r\n",
    "\r\n",
    "    def neural_net_mlp(self,verbose=0):\r\n",
    "        print('\\nTraining neural network: ')\r\n",
    "        # train neural network\r\n",
    "        nn_regres = MLPRegressor(hidden_layer_sizes=(int(self.lag_window_length*2)),shuffle=False,random_state=1, \r\n",
    "                                max_iter=1000,verbose=verbose).fit(self.X_train,self.y_train)\r\n",
    "\r\n",
    "        # make predictions\r\n",
    "        nn_predictions = nn_regres.predict(self.X_test)\r\n",
    "\r\n",
    "        # evaluate\r\n",
    "        mse = mean_squared_error(self.y_test,nn_predictions[:])\r\n",
    "        mae = mean_absolute_error(self.y_test,nn_predictions[:])\r\n",
    "\r\n",
    "        print('RMSE: ',np.sqrt(mse))\r\n",
    "        print('MAE: ',mae)\r\n",
    "\r\n",
    "        # save predictions\r\n",
    "        self.neural_net_predictions = nn_predictions\r\n",
    "\r\n",
    "    def lstm(self):\r\n",
    "        print('\\nTraining LSTM: ')\r\n",
    "\r\n",
    "        # transform data\r\n",
    "        trainX = np.reshape(self.X_train, (self.X_train.shape[0], 1, self.X_train.shape[1]))\r\n",
    "        testX = np.reshape(self.X_test, (self.X_test.shape[0], 1, self.X_train.shape[1]))\r\n",
    "        trainY = normal.y_train\r\n",
    "        testY = normal.y_test\r\n",
    "\r\n",
    "        # create and fit the LSTM network\r\n",
    "        model = Sequential()\r\n",
    "        model.add(LSTM(4, input_shape=(1, self.lag_window_length)))\r\n",
    "        model.add(Dense(1))\r\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\r\n",
    "        model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\r\n",
    "\r\n",
    "        # make predictions\r\n",
    "        trainPredict = model.predict(trainX)\r\n",
    "        testPredict = model.predict(testX)\r\n",
    "\r\n",
    "        # calculate root mean squared error\r\n",
    "        trainScore = math.sqrt(mean_squared_error(trainY, trainPredict))\r\n",
    "        print('Train Score: %.2f RMSE' % (trainScore))\r\n",
    "        testScore = math.sqrt(mean_squared_error(testY, testPredict))\r\n",
    "        print('Test Score: %.2f RMSE' % (testScore))\r\n",
    "\r\n",
    "        # save predictions\r\n",
    "        self.lstm_predictions = testPredict\r\n",
    "\r\n",
    "    def naive_model(self): # t's prediction is t-1's value, note that this means you miss the first time point\r\n",
    "        preds = np.zeros(len(self.one_d_time_series)-1)\r\n",
    "        preds[0] = np.nan()\r\n",
    "        preds[1:] = self.one_d_time_series[0:-2]\r\n",
    "        self.naive_predictions = preds\r\n",
    "\r\n",
    "# ****************************************************************************************************************\r\n",
    "    # visualize results\r\n",
    "# ****************************************************************************************************************\r\n",
    "    def error(self,real_data,predicted_data):\r\n",
    "        error = np.zeros(len(real_data))\r\n",
    "        error = (real_data - predicted_data) / real_data\r\n",
    "        return error\r\n",
    "\r\n",
    "    # visualize orignal time series signal aswell as predictions    \r\n",
    "    def vis_results_time_series(self,second_plot='error'):\r\n",
    "        # plot prediction against actual + training data\r\n",
    "        fig, ax = plt.subplots(2,1,figsize=(10,7),sharex=True)\r\n",
    "\r\n",
    "        # original time series\r\n",
    "        ax[0].plot(self.time_series_dates[self.training_split+self.lag_window_length:],self.one_d_time_series[self.training_split+self.lag_window_length:],'o-',linewidth=3,label='real values',markersize=5) \r\n",
    "\r\n",
    "        # predicted y values\r\n",
    "        ax[0].plot(self.time_series_dates[self.training_split+self.lag_window_length:],self.linear_reg_predictions,'o-',label='linear regression prediction',markersize=5)\r\n",
    "        # ax[0].plot(self.time_series_dates,self.naive_predictions,'.--',label='naive prediction',markersize=5)\r\n",
    "        ax[0].plot(self.time_series_dates[self.training_split+self.lag_window_length:],self.svm_predictions,'.--',label='svm prediction',markersize=5)\r\n",
    "        ax[0].plot(self.time_series_dates[self.training_split+self.lag_window_length:],self.neural_net_predictions,'.--',label='nn prediction',markersize=5)\r\n",
    "        ax[0].plot(self.time_series_dates[self.training_split+self.lag_window_length:],self.lstm_predictions,'.--',label='lstm prediction',markersize=5)\r\n",
    "\r\n",
    "        ax[0].legend()\r\n",
    "        ax[0].set_title('Real values vs model predictions')\r\n",
    "\r\n",
    "        # plot error plot\r\n",
    "        if second_plot == 'error':\r\n",
    "            error_linreg = self.error(self.y_test,self.linear_reg_predictions)\r\n",
    "            # error_naive = error(np.array(test_data[:,-1]),naive_predictions)\r\n",
    "            error_svm = self.error(self.y_test,self.svm_predictions)\r\n",
    "            error_nn = self.error(self.y_test,self.neural_net_predictions)\r\n",
    "            error_lstm = self.error(self.y_test,self.lstm_predictions)\r\n",
    "\r\n",
    "            ax[1].plot(self.time_series_dates[self.training_split+self.lag_window_length:],error_linreg,'r-',label='linear reg error')\r\n",
    "            # ax[1].plot(self.time_series_dates,error_naive[1:],'-',label='naive error')\r\n",
    "            ax[1].plot(self.time_series_dates[self.training_split+self.lag_window_length:],error_svm,'-',label='svm error')\r\n",
    "            ax[1].plot(self.time_series_dates[self.training_split+self.lag_window_length:],error_nn,'-',label='nn error')\r\n",
    "            # ax[1].plot(self.time_series_dates[self.training_split+self.lag_window_length:],error_lstm,'-',label='lstm error')\r\n",
    "\r\n",
    "            ax[1].set_title('Error signal for predictive models')\r\n",
    "            ax[1].set_xlabel('Dates')\r\n",
    "            ax[1].legend()\r\n",
    "            # ax[1].set_ylim([-10,10])\r\n",
    "            ax[1].set_xticks([self.time_series_dates[x] for x in range(self.training_split,len(self.time_series_dates),28)])\r\n",
    "            ax[1].tick_params(rotation=30)\r\n",
    "        \r\n",
    "        elif second_plot == 'cumprod':\r\n",
    "\r\n",
    "            # plot cummulative prod plots - this should only be done if input data is percentage retunrs\r\n",
    "            self.real_vals_cumprod = (self.y_test+1).cumprod()\r\n",
    "            self.linear_reg_predictions_cumprod = (self.linear_reg_predictions + 1).cumprod()\r\n",
    "            self.svm_predictions_cumprod = (self.svm_predictions + 1).cumprod()\r\n",
    "            self.neural_net_predictions_cumprod = (self.neural_net_predictions + 1).cumprod()\r\n",
    "\r\n",
    "            ax[1].plot(self.time_series_dates[self.training_split+self.lag_window_length:],self.real_vals_cumprod,'-',label='real vals cumprod')\r\n",
    "            ax[1].plot(self.time_series_dates[self.training_split+self.lag_window_length:],self.linear_reg_predictions_cumprod,'-',label='linear reg cumprod')\r\n",
    "            ax[1].plot(self.time_series_dates[self.training_split+self.lag_window_length:],self.svm_predictions_cumprod,'-',label='svm cumprod')\r\n",
    "            ax[1].plot(self.time_series_dates[self.training_split+self.lag_window_length:],self.neural_net_predictions_cumprod,'-',label='nn cumprod')\r\n",
    "\r\n",
    "            ax[1].set_xticks([self.time_series_dates[x] for x in range(self.training_split,len(self.time_series_dates),28)])\r\n",
    "            ax[1].tick_params(rotation=30)\r\n",
    "            ax[1].legend()\r\n",
    "\r\n",
    "        # titles and save figures\r\n",
    "        # title_string = 'S&P500 predictions _ y is '+str(column)+'_ window len is '+ str(window_length)\r\n",
    "        # fig.suptitle(title_string)\r\n",
    "        \r\n",
    "        # fig_name = '../results/univariate_single_step_ahead/'+title_string+'.png'\r\n",
    "        # plt.savefig(fig_name,facecolor='w')\r\n",
    "        plt.tight_layout()\r\n",
    "\r\n",
    "    # visualize predictions against real values using scatter plot\r\n",
    "    def vis_results_scatter(self):\r\n",
    "\r\n",
    "        # create dataframe to hold all results\r\n",
    "        df_predictions = pd.DataFrame(index=self.time_series_dates[self.training_split+self.lag_window_length:],columns=['Real_values','linear_reg_predictions','svm_predictions','neural_net_predictions'])\r\n",
    "        df_predictions['Real_values'] = self.y_test\r\n",
    "        df_predictions['linear_reg_predictions'] = self.linear_reg_predictions\r\n",
    "        df_predictions['svm_predictions'] = self.svm_predictions\r\n",
    "        df_predictions['neural_net_predictions'] = self.neural_net_predictions\r\n",
    "\r\n",
    "        # scatter plot with hues\r\n",
    "        fig, ax = plt.subplots(3,1,figsize=(7,10))\r\n",
    "        sns.scatterplot(y=df_predictions['Real_values'],x=df_predictions['linear_reg_predictions'],ax=ax[0])\r\n",
    "        sns.lineplot(x=self.y_test,y=self.y_test,ax=ax[0],color='red')\r\n",
    "\r\n",
    "        sns.scatterplot(y=df_predictions['Real_values'],x=df_predictions['svm_predictions'],ax=ax[1])\r\n",
    "        sns.lineplot(x=self.y_test,y=self.y_test,ax=ax[1],color='red')\r\n",
    "\r\n",
    "        sns.scatterplot(y=df_predictions['Real_values'],x=df_predictions['neural_net_predictions'],ax=ax[2])\r\n",
    "        sns.lineplot(x=self.y_test,y=self.y_test,ax=ax[2],color='red')\r\n",
    "\r\n",
    "        # plot formatting\r\n",
    "        plt.tight_layout()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "########################################################################\r\n",
    "# data\r\n",
    "########################################################################\r\n",
    "df = pd.read_csv('./test_data/AirPassengers.csv')\r\n",
    "x = sp_500['#Passengers']#[-4000:]\r\n",
    "dates = sp_500['Month']#[-4000:]\r\n",
    "\r\n",
    "# percentage returns\r\n",
    "x_pct = x.pct_change().fillna(0)\r\n",
    "x_pct\r\n",
    "\r\n",
    "# create new df hold both\r\n",
    "# df = pd.DataFrame(columns=['Dates','Open','pct_change','pct_change_cumprod']) # ,'log_transform'\r\n",
    "# df['Month'] = dates\r\n",
    "# df['Volume'] =  x\r\n",
    "# df['pct_change'] = x_pct\r\n",
    "# df['pct_change_cumprod'] = (x_pct + 1).cumprod()\r\n",
    "\r\n",
    "########################################################################\r\n",
    "# initialize class object\r\n",
    "########################################################################\r\n",
    "normal = time_series_prediction(df['Month'],df['#Passengers'],10,1) # pass time series, lag window length, a number of steps ahead to predict\r\n",
    "normal.sliding_window_1(verbose=0) # time series to supervised learning ML problem\r\n",
    "normal.train_test_split(split=100) # testing and training dataset split\r\n",
    "normal.test_train_plot()    # visualize training split\r\n",
    "\r\n",
    "########################################################################\r\n",
    "# perform some prediction tasks\r\n",
    "########################################################################\r\n",
    "normal.linear_regression()\r\n",
    "normal.support_vector_machine()\r\n",
    "normal.neural_net_mlp()\r\n",
    "normal.lstm()\r\n",
    "\r\n",
    "\r\n",
    "normal.vis_results_time_series(second_plot='error')"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "16e88070381d451886f073ed588c4187"
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training multivariate linear regression:\n",
      "\n",
      "Linear regression coefficients: \n",
      " [ 0.26215382  0.2450409  -0.27074476 -0.02019418 -0.07375448  0.38694387\n",
      " -0.42111804  0.23977833 -0.39626002  1.0571671 ]\n",
      "RMSE:  37.051892047287666\n",
      "MAE:  31.448017880592797\n",
      "\n",
      "Training support vector machine:\n",
      "RMSE:  43.547451577664276\n",
      "MAE:  33.15561210470844\n",
      "\n",
      "Training neural network: \n",
      "RMSE:  88.23679464192749\n",
      "MAE:  78.89174629726706\n",
      "\n",
      "Training LSTM: \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\tristan\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "100/100 - 9s - loss: 65952.0781\n",
      "Epoch 2/100\n",
      "100/100 - 0s - loss: 65696.8438\n",
      "Epoch 3/100\n",
      "100/100 - 0s - loss: 65616.4375\n",
      "Epoch 4/100\n",
      "100/100 - 0s - loss: 65535.7656\n",
      "Epoch 5/100\n",
      "100/100 - 0s - loss: 65455.3008\n",
      "Epoch 6/100\n",
      "100/100 - 0s - loss: 65374.7852\n",
      "Epoch 7/100\n",
      "100/100 - 0s - loss: 65294.5195\n",
      "Epoch 8/100\n",
      "100/100 - 0s - loss: 65214.1797\n",
      "Epoch 9/100\n",
      "100/100 - 0s - loss: 65134.0117\n",
      "Epoch 10/100\n",
      "100/100 - 0s - loss: 65053.5781\n",
      "Epoch 11/100\n",
      "100/100 - 0s - loss: 64973.6094\n",
      "Epoch 12/100\n",
      "100/100 - 0s - loss: 64893.3555\n",
      "Epoch 13/100\n",
      "100/100 - 0s - loss: 64813.4492\n",
      "Epoch 14/100\n",
      "100/100 - 0s - loss: 64733.4609\n",
      "Epoch 15/100\n",
      "100/100 - 0s - loss: 64653.6602\n",
      "Epoch 16/100\n",
      "100/100 - 0s - loss: 64573.7734\n",
      "Epoch 17/100\n",
      "100/100 - 0s - loss: 64493.9258\n",
      "Epoch 18/100\n",
      "100/100 - 0s - loss: 64414.3281\n",
      "Epoch 19/100\n",
      "100/100 - 0s - loss: 64334.7305\n",
      "Epoch 20/100\n",
      "100/100 - 0s - loss: 64254.9492\n",
      "Epoch 21/100\n",
      "100/100 - 0s - loss: 64175.6250\n",
      "Epoch 22/100\n",
      "100/100 - 0s - loss: 64096.0469\n",
      "Epoch 23/100\n",
      "100/100 - 0s - loss: 64016.7695\n",
      "Epoch 24/100\n",
      "100/100 - 0s - loss: 63937.4766\n",
      "Epoch 25/100\n",
      "100/100 - 0s - loss: 63858.4219\n",
      "Epoch 26/100\n",
      "100/100 - 0s - loss: 63779.2461\n",
      "Epoch 27/100\n",
      "100/100 - 0s - loss: 63700.3438\n",
      "Epoch 28/100\n",
      "100/100 - 0s - loss: 63621.3711\n",
      "Epoch 29/100\n",
      "100/100 - 0s - loss: 63542.7031\n",
      "Epoch 30/100\n",
      "100/100 - 0s - loss: 63463.9141\n",
      "Epoch 31/100\n",
      "100/100 - 0s - loss: 63385.1133\n",
      "Epoch 32/100\n",
      "100/100 - 0s - loss: 63306.4805\n",
      "Epoch 33/100\n",
      "100/100 - 0s - loss: 63227.9453\n",
      "Epoch 34/100\n",
      "100/100 - 0s - loss: 63149.3047\n",
      "Epoch 35/100\n",
      "100/100 - 0s - loss: 63070.8945\n",
      "Epoch 36/100\n",
      "100/100 - 0s - loss: 62992.4531\n",
      "Epoch 37/100\n",
      "100/100 - 0s - loss: 62914.1289\n",
      "Epoch 38/100\n",
      "100/100 - 0s - loss: 62835.8438\n",
      "Epoch 39/100\n",
      "100/100 - 0s - loss: 62757.5586\n",
      "Epoch 40/100\n",
      "100/100 - 0s - loss: 62679.4805\n",
      "Epoch 41/100\n",
      "100/100 - 0s - loss: 62601.3555\n",
      "Epoch 42/100\n",
      "100/100 - 0s - loss: 62523.2461\n",
      "Epoch 43/100\n",
      "100/100 - 0s - loss: 62445.2461\n",
      "Epoch 44/100\n",
      "100/100 - 0s - loss: 62367.3984\n",
      "Epoch 45/100\n",
      "100/100 - 0s - loss: 62289.5508\n",
      "Epoch 46/100\n",
      "100/100 - 0s - loss: 62211.7734\n",
      "Epoch 47/100\n",
      "100/100 - 0s - loss: 62134.0664\n",
      "Epoch 48/100\n",
      "100/100 - 0s - loss: 62056.4805\n",
      "Epoch 49/100\n",
      "100/100 - 0s - loss: 61978.8398\n",
      "Epoch 50/100\n",
      "100/100 - 0s - loss: 61901.3008\n",
      "Epoch 51/100\n",
      "100/100 - 0s - loss: 61823.9141\n",
      "Epoch 52/100\n",
      "100/100 - 0s - loss: 61746.5859\n",
      "Epoch 53/100\n",
      "100/100 - 0s - loss: 61669.2695\n",
      "Epoch 54/100\n",
      "100/100 - 0s - loss: 61592.0547\n",
      "Epoch 55/100\n",
      "100/100 - 0s - loss: 61514.8438\n",
      "Epoch 56/100\n",
      "100/100 - 0s - loss: 61437.8555\n",
      "Epoch 57/100\n",
      "100/100 - 0s - loss: 61360.6094\n",
      "Epoch 58/100\n",
      "100/100 - 0s - loss: 61283.7617\n",
      "Epoch 59/100\n",
      "100/100 - 0s - loss: 61206.7852\n",
      "Epoch 60/100\n",
      "100/100 - 0s - loss: 61129.8945\n",
      "Epoch 61/100\n",
      "100/100 - 0s - loss: 61053.0547\n",
      "Epoch 62/100\n",
      "100/100 - 0s - loss: 60976.2734\n",
      "Epoch 63/100\n",
      "100/100 - 0s - loss: 60899.5703\n",
      "Epoch 64/100\n",
      "100/100 - 0s - loss: 60822.8516\n",
      "Epoch 65/100\n",
      "100/100 - 0s - loss: 60746.1406\n",
      "Epoch 66/100\n",
      "100/100 - 0s - loss: 60669.6641\n",
      "Epoch 67/100\n",
      "100/100 - 0s - loss: 60593.1719\n",
      "Epoch 68/100\n",
      "100/100 - 0s - loss: 60516.6914\n",
      "Epoch 69/100\n",
      "100/100 - 0s - loss: 60440.3867\n",
      "Epoch 70/100\n",
      "100/100 - 0s - loss: 60364.1641\n",
      "Epoch 71/100\n",
      "100/100 - 0s - loss: 60287.9297\n",
      "Epoch 72/100\n",
      "100/100 - 0s - loss: 60211.7695\n",
      "Epoch 73/100\n",
      "100/100 - 0s - loss: 60135.5547\n",
      "Epoch 74/100\n",
      "100/100 - 0s - loss: 60059.4648\n",
      "Epoch 75/100\n",
      "100/100 - 0s - loss: 59983.6641\n",
      "Epoch 76/100\n",
      "100/100 - 0s - loss: 59907.5391\n",
      "Epoch 77/100\n",
      "100/100 - 0s - loss: 59831.8047\n",
      "Epoch 78/100\n",
      "100/100 - 0s - loss: 59755.7852\n",
      "Epoch 79/100\n",
      "100/100 - 0s - loss: 59680.1602\n",
      "Epoch 80/100\n",
      "100/100 - 0s - loss: 59604.2383\n",
      "Epoch 81/100\n",
      "100/100 - 0s - loss: 59528.7617\n",
      "Epoch 82/100\n",
      "100/100 - 0s - loss: 59453.1953\n",
      "Epoch 83/100\n",
      "100/100 - 0s - loss: 59377.6797\n",
      "Epoch 84/100\n",
      "100/100 - 0s - loss: 59302.2344\n",
      "Epoch 85/100\n",
      "100/100 - 0s - loss: 59226.7852\n",
      "Epoch 86/100\n",
      "100/100 - 0s - loss: 59151.7500\n",
      "Epoch 87/100\n",
      "100/100 - 0s - loss: 59076.2148\n",
      "Epoch 88/100\n",
      "100/100 - 0s - loss: 59001.2891\n",
      "Epoch 89/100\n",
      "100/100 - 0s - loss: 58925.9805\n",
      "Epoch 90/100\n",
      "100/100 - 0s - loss: 58850.9219\n",
      "Epoch 91/100\n",
      "100/100 - 0s - loss: 58776.0000\n",
      "Epoch 92/100\n",
      "100/100 - 0s - loss: 58701.1211\n",
      "Epoch 93/100\n",
      "100/100 - 0s - loss: 58626.1094\n",
      "Epoch 94/100\n",
      "100/100 - 0s - loss: 58551.3203\n",
      "Epoch 95/100\n",
      "100/100 - 0s - loss: 58476.5312\n",
      "Epoch 96/100\n",
      "100/100 - 0s - loss: 58401.8047\n",
      "Epoch 97/100\n",
      "100/100 - 0s - loss: 58327.0859\n",
      "Epoch 98/100\n",
      "100/100 - 0s - loss: 58252.5859\n",
      "Epoch 99/100\n",
      "100/100 - 0s - loss: 58178.0469\n",
      "Epoch 100/100\n",
      "100/100 - 0s - loss: 58103.5000\n",
      "Train Score: 240.97 RMSE\n",
      "Test Score: 424.65 RMSE\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ab69994e0144b05b45e3bb67b9f42d5"
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "sp_500"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>#Passengers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1949-01</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1949-02</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1949-03</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1949-04</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1949-05</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>1960-08</td>\n",
       "      <td>606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>1960-09</td>\n",
       "      <td>508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>1960-10</td>\n",
       "      <td>461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>1960-11</td>\n",
       "      <td>390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>1960-12</td>\n",
       "      <td>432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Month  #Passengers\n",
       "0    1949-01          112\n",
       "1    1949-02          118\n",
       "2    1949-03          132\n",
       "3    1949-04          129\n",
       "4    1949-05          121\n",
       "..       ...          ...\n",
       "139  1960-08          606\n",
       "140  1960-09          508\n",
       "141  1960-10          461\n",
       "142  1960-11          390\n",
       "143  1960-12          432\n",
       "\n",
       "[144 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build lstm"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "trainX = np.reshape(normal.X_train, (normal.X_train.shape[0], 1, normal.X_train.shape[1]))\r\n",
    "testX = np.reshape(normal.X_test, (normal.X_test.shape[0], 1, normal.X_train.shape[1]))\r\n",
    "trainY = normal.y_train\r\n",
    "testY = normal.y_test\r\n",
    "\r\n",
    "# create and fit the LSTM network\r\n",
    "model = Sequential()\r\n",
    "model.add(LSTM(4, input_shape=(1, 5)))\r\n",
    "model.add(Dense(1))\r\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\r\n",
    "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\r\n",
    "\r\n",
    "# make predictions\r\n",
    "trainPredict = model.predict(trainX)\r\n",
    "testPredict = model.predict(testX)\r\n",
    "\r\n",
    "# calculate root mean squared error\r\n",
    "trainScore = math.sqrt(mean_squared_error(trainY, trainPredict))\r\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\r\n",
    "testScore = math.sqrt(mean_squared_error(testY, testPredict))\r\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "3500/3500 - 2s - loss: 1.3003\n",
      "Epoch 2/100\n",
      "3500/3500 - 1s - loss: 0.4772\n",
      "Epoch 3/100\n",
      "3500/3500 - 1s - loss: 0.4081\n",
      "Epoch 4/100\n",
      "3500/3500 - 1s - loss: 0.3913\n",
      "Epoch 5/100\n",
      "3500/3500 - 1s - loss: 0.3805\n",
      "Epoch 6/100\n",
      "3500/3500 - 1s - loss: 0.3764\n",
      "Epoch 7/100\n",
      "3500/3500 - 1s - loss: 0.3747\n",
      "Epoch 8/100\n",
      "3500/3500 - 1s - loss: 0.3765\n",
      "Epoch 9/100\n",
      "3500/3500 - 1s - loss: 0.3737\n",
      "Epoch 10/100\n",
      "3500/3500 - 1s - loss: 0.3764\n",
      "Epoch 11/100\n",
      "3500/3500 - 1s - loss: 0.3683\n",
      "Epoch 12/100\n",
      "3500/3500 - 1s - loss: 0.3710\n",
      "Epoch 13/100\n",
      "3500/3500 - 1s - loss: 0.3645\n",
      "Epoch 14/100\n",
      "3500/3500 - 1s - loss: 0.3699\n",
      "Epoch 15/100\n",
      "3500/3500 - 1s - loss: 0.3657\n",
      "Epoch 16/100\n",
      "3500/3500 - 1s - loss: 0.3639\n",
      "Epoch 17/100\n",
      "3500/3500 - 1s - loss: 0.3678\n",
      "Epoch 18/100\n",
      "3500/3500 - 1s - loss: 0.3634\n",
      "Epoch 19/100\n",
      "3500/3500 - 1s - loss: 0.3622\n",
      "Epoch 20/100\n",
      "3500/3500 - 1s - loss: 0.3590\n",
      "Epoch 21/100\n",
      "3500/3500 - 1s - loss: 0.3586\n",
      "Epoch 22/100\n",
      "3500/3500 - 1s - loss: 0.3630\n",
      "Epoch 23/100\n",
      "3500/3500 - 1s - loss: 0.3635\n",
      "Epoch 24/100\n",
      "3500/3500 - 1s - loss: 0.3624\n",
      "Epoch 25/100\n",
      "3500/3500 - 1s - loss: 0.3590\n",
      "Epoch 26/100\n",
      "3500/3500 - 1s - loss: 0.3628\n",
      "Epoch 27/100\n",
      "3500/3500 - 1s - loss: 0.3608\n",
      "Epoch 28/100\n",
      "3500/3500 - 1s - loss: 0.3566\n",
      "Epoch 29/100\n",
      "3500/3500 - 1s - loss: 0.3582\n",
      "Epoch 30/100\n",
      "3500/3500 - 1s - loss: 0.3601\n",
      "Epoch 31/100\n",
      "3500/3500 - 1s - loss: 0.3577\n",
      "Epoch 32/100\n",
      "3500/3500 - 1s - loss: 0.3574\n",
      "Epoch 33/100\n",
      "3500/3500 - 1s - loss: 0.3579\n",
      "Epoch 34/100\n",
      "3500/3500 - 1s - loss: 0.3552\n",
      "Epoch 35/100\n",
      "3500/3500 - 1s - loss: 0.3585\n",
      "Epoch 36/100\n",
      "3500/3500 - 1s - loss: 0.3582\n",
      "Epoch 37/100\n",
      "3500/3500 - 1s - loss: 0.3558\n",
      "Epoch 38/100\n",
      "3500/3500 - 1s - loss: 0.3570\n",
      "Epoch 39/100\n",
      "3500/3500 - 1s - loss: 0.3529\n",
      "Epoch 40/100\n",
      "3500/3500 - 1s - loss: 0.3523\n",
      "Epoch 41/100\n",
      "3500/3500 - 1s - loss: 0.3550\n",
      "Epoch 42/100\n",
      "3500/3500 - 1s - loss: 0.3533\n",
      "Epoch 43/100\n",
      "3500/3500 - 1s - loss: 0.3539\n",
      "Epoch 44/100\n",
      "3500/3500 - 1s - loss: 0.3501\n",
      "Epoch 45/100\n",
      "3500/3500 - 1s - loss: 0.3516\n",
      "Epoch 46/100\n",
      "3500/3500 - 1s - loss: 0.3577\n",
      "Epoch 47/100\n",
      "3500/3500 - 1s - loss: 0.3518\n",
      "Epoch 48/100\n",
      "3500/3500 - 1s - loss: 0.3523\n",
      "Epoch 49/100\n",
      "3500/3500 - 1s - loss: 0.3531\n",
      "Epoch 50/100\n",
      "3500/3500 - 1s - loss: 0.3520\n",
      "Epoch 51/100\n",
      "3500/3500 - 1s - loss: 0.3548\n",
      "Epoch 52/100\n",
      "3500/3500 - 1s - loss: 0.3503\n",
      "Epoch 53/100\n",
      "3500/3500 - 1s - loss: 0.3505\n",
      "Epoch 54/100\n",
      "3500/3500 - 1s - loss: 0.3538\n",
      "Epoch 55/100\n",
      "3500/3500 - 1s - loss: 0.3530\n",
      "Epoch 56/100\n",
      "3500/3500 - 1s - loss: 0.3509\n",
      "Epoch 57/100\n",
      "3500/3500 - 1s - loss: 0.3511\n",
      "Epoch 58/100\n",
      "3500/3500 - 1s - loss: 0.3542\n",
      "Epoch 59/100\n",
      "3500/3500 - 1s - loss: 0.3504\n",
      "Epoch 60/100\n",
      "3500/3500 - 1s - loss: 0.3498\n",
      "Epoch 61/100\n",
      "3500/3500 - 1s - loss: 0.3501\n",
      "Epoch 62/100\n",
      "3500/3500 - 1s - loss: 0.3488\n",
      "Epoch 63/100\n",
      "3500/3500 - 1s - loss: 0.3478\n",
      "Epoch 64/100\n",
      "3500/3500 - 1s - loss: 0.3481\n",
      "Epoch 65/100\n",
      "3500/3500 - 1s - loss: 0.3479\n",
      "Epoch 66/100\n",
      "3500/3500 - 1s - loss: 0.3507\n",
      "Epoch 67/100\n",
      "3500/3500 - 1s - loss: 0.3508\n",
      "Epoch 68/100\n",
      "3500/3500 - 1s - loss: 0.3493\n",
      "Epoch 69/100\n",
      "3500/3500 - 1s - loss: 0.3479\n",
      "Epoch 70/100\n",
      "3500/3500 - 1s - loss: 0.3461\n",
      "Epoch 71/100\n",
      "3500/3500 - 1s - loss: 0.3494\n",
      "Epoch 72/100\n",
      "3500/3500 - 1s - loss: 0.3504\n",
      "Epoch 73/100\n",
      "3500/3500 - 1s - loss: 0.3488\n",
      "Epoch 74/100\n",
      "3500/3500 - 1s - loss: 0.3488\n",
      "Epoch 75/100\n",
      "3500/3500 - 1s - loss: 0.3521\n",
      "Epoch 76/100\n",
      "3500/3500 - 1s - loss: 0.3496\n",
      "Epoch 77/100\n",
      "3500/3500 - 1s - loss: 0.3464\n",
      "Epoch 78/100\n",
      "3500/3500 - 1s - loss: 0.3501\n",
      "Epoch 79/100\n",
      "3500/3500 - 1s - loss: 0.3463\n",
      "Epoch 80/100\n",
      "3500/3500 - 1s - loss: 0.3480\n",
      "Epoch 81/100\n",
      "3500/3500 - 1s - loss: 0.3465\n",
      "Epoch 82/100\n",
      "3500/3500 - 1s - loss: 0.3501\n",
      "Epoch 83/100\n",
      "3500/3500 - 1s - loss: 0.3457\n",
      "Epoch 84/100\n",
      "3500/3500 - 1s - loss: 0.3480\n",
      "Epoch 85/100\n",
      "3500/3500 - 1s - loss: 0.3499\n",
      "Epoch 86/100\n",
      "3500/3500 - 1s - loss: 0.3457\n",
      "Epoch 87/100\n",
      "3500/3500 - 1s - loss: 0.3486\n",
      "Epoch 88/100\n",
      "3500/3500 - 1s - loss: 0.3464\n",
      "Epoch 89/100\n",
      "3500/3500 - 1s - loss: 0.3451\n",
      "Epoch 90/100\n",
      "3500/3500 - 1s - loss: 0.3440\n",
      "Epoch 91/100\n",
      "3500/3500 - 1s - loss: 0.3482\n",
      "Epoch 92/100\n",
      "3500/3500 - 1s - loss: 0.3475\n",
      "Epoch 93/100\n",
      "3500/3500 - 1s - loss: 0.3486\n",
      "Epoch 94/100\n",
      "3500/3500 - 1s - loss: 0.3472\n",
      "Epoch 95/100\n",
      "3500/3500 - 1s - loss: 0.3484\n",
      "Epoch 96/100\n",
      "3500/3500 - 1s - loss: 0.3443\n",
      "Epoch 97/100\n",
      "3500/3500 - 1s - loss: 0.3439\n",
      "Epoch 98/100\n",
      "3500/3500 - 1s - loss: 0.3465\n",
      "Epoch 99/100\n",
      "3500/3500 - 1s - loss: 0.3468\n",
      "Epoch 100/100\n",
      "3500/3500 - 1s - loss: 0.3425\n",
      "Train Score: 0.61 RMSE\n",
      "Test Score: 0.56 RMSE\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "trainPredict"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[2.7830205],\n",
       "       [2.6712413],\n",
       "       [2.2477574],\n",
       "       ...,\n",
       "       [3.8193555],\n",
       "       [3.7082005],\n",
       "       [3.7810779]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "1"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-41413e09f43f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnormal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}