{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "e1d4880ad13fa2099fc93eba0cb791232af4f1a31a1c632661aaef6a29f2ead6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Code up class to perform different tasks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive figures\n",
    "%matplotlib widget \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# model evalution metrics\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# data preprocessing\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# predictive models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# cross validation and hyper-parameter search\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "source": [
    "# 1.0 Class for univariate one-step ahead forecasting "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class time_series_prediction():\n",
    "\n",
    "    def __init__(self,dates,one_d_time_series,lag_window_length,n_ahead_prediction):\n",
    "\n",
    "        # raw input data + settings for time series -> supervised learning ML problem\n",
    "        self.one_d_time_series = np.array(one_d_time_series)      # time series array, to array ensure index works as expected for class methods\n",
    "        self.time_series_dates = np.array(dates)                  # time stamp / date for each data point\n",
    "        self.lag_window_length = lag_window_length                # length of lag window\n",
    "        self.n_ahead_prediction = n_ahead_prediction              # time ahead to predict\n",
    "\n",
    "        # transfromed data: set after calling .sliding_window_1()\n",
    "        self.input_data = None\n",
    "        self.target_data = None\n",
    "\n",
    "        # testing and training data: set after calling .train_test_split()\n",
    "        self.training_split = None\n",
    "        self.X_test = None\n",
    "        self.X_train = None\n",
    "        self.y_test = None\n",
    "        self.y_train = None\n",
    "\n",
    "        # predictions from various models - set after calling each models training\n",
    "        self.linear_reg_predictions = None\n",
    "        self.svm_predictions = None\n",
    "        self.neural_net_predictions = None\n",
    "        self.naive_predictions = None\n",
    "\n",
    "        # cumprod results from predictions - set after calling .vis_results_time_series()\n",
    "        self.real_vals_cumprod = None\n",
    "        self.linear_reg_predictions_cumprod = None\n",
    "        self.svm_predictions_cumprod = None\n",
    "        self.neural_net_predictions_cumprod = None\n",
    "    \n",
    "\n",
    "# ****************************************************************************************************************\n",
    "    # data wrangling\n",
    "# ****************************************************************************************************************\n",
    "\n",
    "    # method to transfroms 1-D time series to supervised ML problem: one step ahead forecasting   \n",
    "    def sliding_window_1(self,verbose):\n",
    "        # initialize input array\n",
    "        num_rows = len(self.one_d_time_series) - self.lag_window_length\n",
    "        array = np.zeros((num_rows, self.lag_window_length + 1))\n",
    "        \n",
    "        # loop through data and populate array\n",
    "        for i in range(num_rows):\n",
    "            # input features\n",
    "            array[i,0:self.lag_window_length+1] = self.one_d_time_series[i:i+self.lag_window_length+1]\n",
    "            # target feature/s\n",
    "            array[i,-1] = self.one_d_time_series[i+self.lag_window_length]\n",
    "            \n",
    "            if verbose == 1:\n",
    "                # show pattern\n",
    "                print(array[i,0:self.lag_window_length],' : ',array[i,self.lag_window_length])\n",
    "\n",
    "        # save results as a class attribute\n",
    "        self.input_data = array[:,0:self.lag_window_length]\n",
    "        self.target_data = array[:,self.lag_window_length]\n",
    "\n",
    "    # method to perform a training and testing split for dataset with only a single column of target variables\n",
    "    def train_test_split(self,split):\n",
    "        # sequentially splits data for testing and training\n",
    "        self.training_split = split\n",
    "        self.X_train = self.input_data[0:split,:]\n",
    "        self.X_test = self.input_data[split:,:]\n",
    "        self.y_train = self.target_data[0:split]\n",
    "        self.y_test = self.target_data[split:]\n",
    "\n",
    "        # generate different folds from training data for cross validation during hyperparameter tuning\n",
    "\n",
    "        # different folds for cross validation\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "        # visualize cross validation splits\n",
    "        fig,ax = plt.subplots(5,1,sharex=True)\n",
    "        i = 0\n",
    "        training_data = self.one_d_time_series[0:self.training_split]\n",
    "        for tr_index, val_index in tscv.split(training_data): # training and validation splits for 5 folds\n",
    "            # print(tr_index, val_index)\n",
    "            ax[i].plot(tr_index,training_data[tr_index[0]:tr_index[-1]+1],'b-',label='training set')\n",
    "            ax[i].plot(val_index,training_data[val_index[0]:val_index[-1]+1],'r-',label='validation set')\n",
    "            ax[i].legend()\n",
    "            i += 1\n",
    "        ax[0].set_title('Cross validation sets for hyperparameter tuning')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ****************************************************************************************************************\n",
    "    # predictive models\n",
    "# ****************************************************************************************************************\n",
    "\n",
    "    def linear_regression(self):\n",
    "        print('Training multivariate linear regression:')\n",
    "        # train model\n",
    "        reg_model = LinearRegression().fit(self.X_train,self.y_train)\n",
    "        print('\\nLinear regression coefficients: \\n',reg_model.coef_)\n",
    "\n",
    "        # test model\n",
    "        predictions = reg_model.predict(self.X_test)\n",
    "\n",
    "        # evaluate: use sklearn metric methods to calc rmse and mae\n",
    "        mse = mean_squared_error(self.y_test,predictions)\n",
    "        mae = mean_absolute_error(self.y_test,predictions)\n",
    "\n",
    "        print('RMSE: ',np.sqrt(mse))\n",
    "        print('MAE: ',mae)\n",
    "\n",
    "        # save predictions\n",
    "        self.linear_reg_predictions = predictions\n",
    "\n",
    "    def support_vector_machine(self,model_tunning=True,C=None,kernel=None,epsilon=None):\n",
    "        print('\\nTraining support vector machine:')\n",
    "\n",
    "        if model_tunning == False: #hyperparameter are known\n",
    "            # train model\n",
    "            svm_regres = SVR(max_iter=1000,C=C, kernel=kernel, epsilon=epsilon).fit(self.X_train,self.y_train)\n",
    "            print('Model params: ', svm_regres.get_params())\n",
    "            # predict on test set\n",
    "            svm_predictions = svm_regres.predict(self.X_test)\n",
    "\n",
    "            # evaluate\n",
    "            mse = mean_squared_error(self.y_test,svm_predictions[:])\n",
    "            mae = mean_absolute_error(self.y_test,svm_predictions[:])\n",
    "\n",
    "            print('RMSE: ',np.sqrt(mse))\n",
    "            print('MAE: ',mae)\n",
    "\n",
    "            # save predictions\n",
    "            self.svm_predictions = svm_predictions\n",
    "        \n",
    "        else: # must hyperparameter tune model\n",
    "\n",
    "            # define model: support vector machine for regression\n",
    "            model = SVR()\n",
    "\n",
    "            # hyperparameter values to check\n",
    "            param_grid = [\n",
    "            {'C': [0.1, 1, 10, 100], 'kernel': ['linear','rbf','sigmoid'],'epsilon':[0.1,1,10,100]},\n",
    "            ]\n",
    "\n",
    "            # perform grid search, using cross validaiton\n",
    "            tscv = TimeSeriesSplit(n_splits=5)\n",
    "            gsearch = GridSearchCV(estimator=model, cv=tscv, param_grid=param_grid, scoring = 'neg_mean_squared_error',verbose=4,n_jobs=-3)\n",
    "            gsearch.fit(self.X_train, self.y_train)\n",
    "            print('best_score: ', gsearch.best_score_)\n",
    "            print('best_model: ', gsearch.best_estimator_)\n",
    "            print('best_params: ',gsearch.best_params_)\n",
    "\n",
    "            # predict on test set\n",
    "            svm_predictions = gsearch.best_estimator_.predict(self.X_test)\n",
    "\n",
    "            # evaluate\n",
    "            mse = mean_squared_error(self.y_test,svm_predictions[:])\n",
    "            mae = mean_absolute_error(self.y_test,svm_predictions[:])\n",
    "\n",
    "            print('RMSE: ',np.sqrt(mse))\n",
    "            print('MAE: ',mae)\n",
    "\n",
    "            # save predictions\n",
    "            self.svm_predictions = svm_predictions\n",
    "\n",
    "    def neural_net_mlp(self,verbose=0,model_tunning=True,hidden_layer_sizes=None,activation=None,learning_rate=None,learning_rate_init=None):\n",
    "        print('\\nTraining neural network: ')\n",
    "\n",
    "        if model_tunning == False:\n",
    "            # train neural network\n",
    "            nn_regres = MLPRegressor(hidden_layer_sizes=hidden_layer_sizes,activation=activation,learning_rate=learning_rate,learning_rate_init=learning_rate_init,shuffle=False,random_state=1,max_iter=1000,verbose=verbose).fit(self.X_train,self.y_train)\n",
    "            print('Model params:', nn_regres.get_params())\n",
    "            # make predictions\n",
    "            nn_predictions = nn_regres.predict(self.X_test)\n",
    "\n",
    "            # evaluate\n",
    "            mse = mean_squared_error(self.y_test,nn_predictions[:])\n",
    "            mae = mean_absolute_error(self.y_test,nn_predictions[:])\n",
    "\n",
    "            print('RMSE: ',np.sqrt(mse))\n",
    "            print('MAE: ',mae)\n",
    "\n",
    "            # save predictions\n",
    "            self.neural_net_predictions = nn_predictions\n",
    "        \n",
    "        else: # perform hyperparameter tuning\n",
    "            MLP = MLPRegressor(shuffle=False,max_iter=1000) # must set shuffle to false to avoid leakage of information due to sequance problem\n",
    "\n",
    "            # hyperparameter values to check\n",
    "            param_grid = [\n",
    "            {'hidden_layer_sizes': [(10,),(100,),(1000,)], 'activation': ['logistic', 'tanh', 'relu'],'learning_rate': ['constant', 'invscaling', 'adaptive'], 'learning_rate_init':[0.001,0.01,1]}\n",
    " ]\n",
    "            # perform grid search, using cross validaiton\n",
    "            tscv = TimeSeriesSplit(n_splits=5)\n",
    "            gsearch = GridSearchCV(estimator=MLP, cv=tscv, param_grid=param_grid, scoring = 'neg_mean_squared_error',verbose=4,n_jobs=-3)\n",
    "            gsearch.fit(self.X_train, self.y_train)\n",
    "            print('best_score: ', gsearch.best_score_)\n",
    "            print('best_model: ', gsearch.best_estimator_)\n",
    "            print('best_params: ',gsearch.best_params_)\n",
    "\n",
    "            # model\n",
    "            mlp_predictions = gsearch.best_estimator_.predict(self.X_test)\n",
    "\n",
    "            # evaluate\n",
    "            mse = mean_squared_error(self.y_test,mlp_predictions)\n",
    "            mae = mean_absolute_error(self.y_test,mlp_predictions)\n",
    "\n",
    "            print('RMSE: ',np.sqrt(mse))\n",
    "            print('MAE: ',mae)\n",
    "\n",
    "             # save predictions\n",
    "            self.neural_net_predictions = mlp_predictions\n",
    "\n",
    "    def naive_model(self): # t's prediction is t-1's value, note that this means you miss the first time point\n",
    "        preds = np.zeros(len(self.one_d_time_series)-1)\n",
    "        preds[0] = np.nan()\n",
    "        preds[1:] = self.one_d_time_series[0:-2]\n",
    "        self.naive_predictions = preds\n",
    "\n",
    "# ****************************************************************************************************************\n",
    "    # visualize results\n",
    "# ****************************************************************************************************************\n",
    "    def error(self,real_data,predicted_data):\n",
    "        error = np.zeros(len(real_data))\n",
    "        error = (real_data - predicted_data) / real_data\n",
    "        return error\n",
    "\n",
    "    # visualize orignal time series signal aswell as predictions    \n",
    "    def vis_results_time_series(self,second_plot='error'):\n",
    "        # plot prediction against actual + training data\n",
    "        fig, ax = plt.subplots(2,1,figsize=(10,7),sharex=True)\n",
    "\n",
    "        # original time series\n",
    "        ax[0].plot(self.time_series_dates[self.training_split+self.lag_window_length:],self.one_d_time_series[self.training_split+self.lag_window_length:],'o-',linewidth=3,label='real values',markersize=5) \n",
    "\n",
    "        # predicted y values\n",
    "        ax[0].plot(self.time_series_dates[self.training_split+self.lag_window_length:],self.linear_reg_predictions,'o-',label='linear regression prediction',markersize=5)\n",
    "        # ax[0].plot(self.time_series_dates,self.naive_predictions,'.--',label='naive prediction',markersize=5)\n",
    "        ax[0].plot(self.time_series_dates[self.training_split+self.lag_window_length:],self.svm_predictions,'.--',label='svm prediction',markersize=5)\n",
    "        ax[0].plot(self.time_series_dates[self.training_split+self.lag_window_length:],self.neural_net_predictions,'.--',label='nn prediction',markersize=5)\n",
    "\n",
    "        ax[0].legend()\n",
    "        ax[0].set_title('Real values vs model predictions')\n",
    "\n",
    "        # plot error plot\n",
    "        if second_plot == 'error':\n",
    "            error_linreg = self.error(self.y_test,self.linear_reg_predictions)\n",
    "            # error_naive = error(np.array(test_data[:,-1]),naive_predictions)\n",
    "            error_svm = self.error(self.y_test,self.svm_predictions)\n",
    "            error_nn = self.error(self.y_test,self.neural_net_predictions)\n",
    "\n",
    "            ax[1].plot(self.time_series_dates[self.training_split+self.lag_window_length:],error_linreg,'r-',label='linear reg error')\n",
    "            # ax[1].plot(self.time_series_dates,error_naive[1:],'-',label='naive error')\n",
    "            ax[1].plot(self.time_series_dates[self.training_split+self.lag_window_length:],error_svm,'-',label='svm error')\n",
    "            ax[1].plot(self.time_series_dates[self.training_split+self.lag_window_length:],error_nn,'-',label='nn error')\n",
    "            ax[1].set_title('Error signal for predictive models')\n",
    "            ax[1].set_xlabel('Dates')\n",
    "            ax[1].legend()\n",
    "            # ax[1].set_ylim([-10,10])\n",
    "            ax[1].set_xticks([self.time_series_dates[x] for x in range(self.training_split,len(self.time_series_dates),28)])\n",
    "            ax[1].tick_params(rotation=30)\n",
    "        \n",
    "        elif second_plot == 'cumprod':\n",
    "\n",
    "            # plot cummulative prod plots - this should only be done if input data is percentage retunrs\n",
    "            self.real_vals_cumprod = (self.y_test+1).cumprod()\n",
    "            self.linear_reg_predictions_cumprod = (self.linear_reg_predictions + 1).cumprod()\n",
    "            self.svm_predictions_cumprod = (self.svm_predictions + 1).cumprod()\n",
    "            self.neural_net_predictions_cumprod = (self.neural_net_predictions + 1).cumprod()\n",
    "\n",
    "            ax[1].plot(self.time_series_dates[self.training_split+self.lag_window_length:],self.real_vals_cumprod,'-',label='real vals cumprod')\n",
    "            ax[1].plot(self.time_series_dates[self.training_split+self.lag_window_length:],self.linear_reg_predictions_cumprod,'-',label='linear reg cumprod')\n",
    "            ax[1].plot(self.time_series_dates[self.training_split+self.lag_window_length:],self.svm_predictions_cumprod,'-',label='svm cumprod')\n",
    "            ax[1].plot(self.time_series_dates[self.training_split+self.lag_window_length:],self.neural_net_predictions_cumprod,'-',label='nn cumprod')\n",
    "\n",
    "            ax[1].set_xticks([self.time_series_dates[x] for x in range(self.training_split,len(self.time_series_dates),28)])\n",
    "            ax[1].tick_params(rotation=30)\n",
    "            ax[1].legend()\n",
    "\n",
    "        # titles and save figures\n",
    "        # title_string = 'S&P500 predictions _ y is '+str(column)+'_ window len is '+ str(window_length)\n",
    "        # fig.suptitle(title_string)\n",
    "        \n",
    "        # fig_name = '../results/univariate_single_step_ahead/'+title_string+'.png'\n",
    "        # plt.savefig(fig_name,facecolor='w')\n",
    "        plt.tight_layout()\n",
    "\n",
    "    # visualize predictions against real values using scatter plot\n",
    "    def vis_results_scatter(self):\n",
    "\n",
    "        # create dataframe to hold all results\n",
    "        df_predictions = pd.DataFrame(index=self.time_series_dates[self.training_split+self.lag_window_length:],columns=['Real_values','linear_reg_predictions','svm_predictions','neural_net_predictions'])\n",
    "        df_predictions['Real_values'] = self.y_test\n",
    "        df_predictions['linear_reg_predictions'] = self.linear_reg_predictions\n",
    "        df_predictions['svm_predictions'] = self.svm_predictions\n",
    "        df_predictions['neural_net_predictions'] = self.neural_net_predictions\n",
    "\n",
    "        # scatter plot with hues\n",
    "        fig, ax = plt.subplots(3,1,figsize=(7,10))\n",
    "        sns.scatterplot(y=df_predictions['Real_values'],x=df_predictions['linear_reg_predictions'],ax=ax[0])\n",
    "        sns.lineplot(x=self.y_test,y=self.y_test,ax=ax[0],color='red')\n",
    "\n",
    "        sns.scatterplot(y=df_predictions['Real_values'],x=df_predictions['svm_predictions'],ax=ax[1])\n",
    "        sns.lineplot(x=self.y_test,y=self.y_test,ax=ax[1],color='red')\n",
    "\n",
    "        sns.scatterplot(y=df_predictions['Real_values'],x=df_predictions['neural_net_predictions'],ax=ax[2])\n",
    "        sns.lineplot(x=self.y_test,y=self.y_test,ax=ax[2],color='red')\n",
    "\n",
    "        # plot formatting\n",
    "        plt.tight_layout()\n",
    "\n",
    "    # method to plot testing and training split of data\n",
    "    def test_train_plot(self):\n",
    "        fig, ax = plt.subplots(figsize=(10,5))\n",
    "        ax.plot(self.time_series_dates[0:self.training_split] ,self.one_d_time_series[0:self.training_split],'k-',label='Training data') # replace returns with sp_500 for other data plotting\n",
    "        ax.plot(self.time_series_dates[self.training_split:] ,self.one_d_time_series[self.training_split:],'r-',label='Testing data')\n",
    "        ax.plot(self.time_series_dates[self.training_split+self.lag_window_length:] ,self.y_test,'o',label='Windowed testing data') # important to match time by start 5 (length of time window) after where segmented our testing and training data\n",
    "        plt.legend(loc=0) \n",
    "        ax.set_xticks([self.time_series_dates[x] for x in range(0,len(self.time_series_dates),150)])\n",
    "        ax.tick_params(rotation=30) \n",
    "        ax.set_title('Test traing split')\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "source": [
    "# 2.0 Import some test data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             Date         Open         High          Low        Close  \\\n",
       "0      1950-01-03    16.660000    16.660000    16.660000    16.660000   \n",
       "1      1950-01-04    16.850000    16.850000    16.850000    16.850000   \n",
       "2      1950-01-05    16.930000    16.930000    16.930000    16.930000   \n",
       "3      1950-01-06    16.980000    16.980000    16.980000    16.980000   \n",
       "4      1950-01-09    17.080000    17.080000    17.080000    17.080000   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "17213  2018-05-31  2720.979980  2722.500000  2700.679932  2705.270020   \n",
       "17214  2018-06-01  2718.699951  2736.929932  2718.699951  2734.620117   \n",
       "17215  2018-06-04  2741.669922  2749.159912  2740.540039  2746.870117   \n",
       "17216  2018-06-05  2748.459961  2752.610107  2739.510010  2748.800049   \n",
       "17217  2018-06-06  2753.250000  2772.389893  2748.459961  2772.350098   \n",
       "\n",
       "         Adj Close      Volume  \n",
       "0        16.660000     1260000  \n",
       "1        16.850000     1890000  \n",
       "2        16.930000     2550000  \n",
       "3        16.980000     2010000  \n",
       "4        17.080000     2520000  \n",
       "...            ...         ...  \n",
       "17213  2705.270020  4235370000  \n",
       "17214  2734.620117  3684130000  \n",
       "17215  2746.870117  3376510000  \n",
       "17216  2748.800049  3517790000  \n",
       "17217  2772.350098  3651640000  \n",
       "\n",
       "[17218 rows x 7 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Adj Close</th>\n      <th>Volume</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1950-01-03</td>\n      <td>16.660000</td>\n      <td>16.660000</td>\n      <td>16.660000</td>\n      <td>16.660000</td>\n      <td>16.660000</td>\n      <td>1260000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1950-01-04</td>\n      <td>16.850000</td>\n      <td>16.850000</td>\n      <td>16.850000</td>\n      <td>16.850000</td>\n      <td>16.850000</td>\n      <td>1890000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1950-01-05</td>\n      <td>16.930000</td>\n      <td>16.930000</td>\n      <td>16.930000</td>\n      <td>16.930000</td>\n      <td>16.930000</td>\n      <td>2550000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1950-01-06</td>\n      <td>16.980000</td>\n      <td>16.980000</td>\n      <td>16.980000</td>\n      <td>16.980000</td>\n      <td>16.980000</td>\n      <td>2010000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1950-01-09</td>\n      <td>17.080000</td>\n      <td>17.080000</td>\n      <td>17.080000</td>\n      <td>17.080000</td>\n      <td>17.080000</td>\n      <td>2520000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>17213</th>\n      <td>2018-05-31</td>\n      <td>2720.979980</td>\n      <td>2722.500000</td>\n      <td>2700.679932</td>\n      <td>2705.270020</td>\n      <td>2705.270020</td>\n      <td>4235370000</td>\n    </tr>\n    <tr>\n      <th>17214</th>\n      <td>2018-06-01</td>\n      <td>2718.699951</td>\n      <td>2736.929932</td>\n      <td>2718.699951</td>\n      <td>2734.620117</td>\n      <td>2734.620117</td>\n      <td>3684130000</td>\n    </tr>\n    <tr>\n      <th>17215</th>\n      <td>2018-06-04</td>\n      <td>2741.669922</td>\n      <td>2749.159912</td>\n      <td>2740.540039</td>\n      <td>2746.870117</td>\n      <td>2746.870117</td>\n      <td>3376510000</td>\n    </tr>\n    <tr>\n      <th>17216</th>\n      <td>2018-06-05</td>\n      <td>2748.459961</td>\n      <td>2752.610107</td>\n      <td>2739.510010</td>\n      <td>2748.800049</td>\n      <td>2748.800049</td>\n      <td>3517790000</td>\n    </tr>\n    <tr>\n      <th>17217</th>\n      <td>2018-06-06</td>\n      <td>2753.250000</td>\n      <td>2772.389893</td>\n      <td>2748.459961</td>\n      <td>2772.350098</td>\n      <td>2772.350098</td>\n      <td>3651640000</td>\n    </tr>\n  </tbody>\n</table>\n<p>17218 rows × 7 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# import some data\n",
    "sp_500 = pd.read_csv('./test_data/GSPC.csv') \n",
    "sp_500"
   ]
  },
  {
   "source": [
    "# 3.0 Example of using class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87514f550bf544ec8cf91caaed9cf7c4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ae99053efa6241b1a8b94e31d3503334"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "# initialize class object\n",
    "normal = time_series_prediction(sp_500['Date'][-2000:],sp_500['Volume'][-2000:]/1e9,5,1) # pass: ime series dates, univariate time series, lag window length, a number of steps ahead to predict\n",
    "normal.sliding_window_1(verbose=0) # time series to supervised ML problem\n",
    "normal.train_test_split(split=1200) # testing and training dataset split\n",
    "normal.test_train_plot()    # visualize training split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training multivariate linear regression:\n",
      "\n",
      "Linear regression coefficients: \n",
      " [ 0.01592332 -0.04798209  0.01470487 -0.03113864  0.01997245 -0.00756195\n",
      "  0.01128548 -0.02102782  0.00054607  0.02279579  0.01401249  0.03641352\n",
      " -0.01377384  0.03288997  0.03230216 -0.05591278  0.02344589  0.02939591\n",
      "  0.01446684 -0.05212056  0.03234467  0.04403855  0.04477377 -0.01877487\n",
      " -0.01662812 -0.04726332  0.03443288 -0.00166518  0.03537087  0.01852641\n",
      " -0.01398799  0.03454426  0.0537333  -0.03814313 -0.01196013 -0.03994896\n",
      "  0.03781376 -0.00117192  0.00285367  0.01492658  0.02168905 -0.00160041\n",
      " -0.01661934 -0.01063986  0.00086509  0.08185323  0.02305453  0.03169653\n",
      "  0.14131047  0.41711918]\n",
      "RMSE:  0.5568350714633039\n",
      "MAE:  0.37348103238908165\n",
      "\n",
      "Training support vector machine:\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 14 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done 150 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-3)]: Done 240 out of 240 | elapsed:   44.4s finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 14 concurrent workers.\n",
      "best_score:  -0.3491967829329086\n",
      "best_model:  SVR(C=0.1, kernel='linear')\n",
      "best_params:  {'C': 0.1, 'epsilon': 0.1, 'kernel': 'linear'}\n",
      "RMSE:  0.564837477948971\n",
      "MAE:  0.3719147967852818\n",
      "\n",
      "Training neural network: \n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "[Parallel(n_jobs=-3)]: Done 140 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-3)]: Done 335 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-3)]: Done 378 out of 405 | elapsed:  1.7min remaining:    7.2s\n",
      "[Parallel(n_jobs=-3)]: Done 405 out of 405 | elapsed:  2.0min finished\n",
      "best_score:  -0.324069643907934\n",
      "best_model:  MLPRegressor(hidden_layer_sizes=(10,), learning_rate='adaptive',\n",
      "             learning_rate_init=0.01, max_iter=1000, shuffle=False)\n",
      "best_params:  {'activation': 'relu', 'hidden_layer_sizes': (10,), 'learning_rate': 'adaptive', 'learning_rate_init': 0.01}\n",
      "RMSE:  0.568030482268734\n",
      "MAE:  0.3842328451557932\n"
     ]
    }
   ],
   "source": [
    "# perform some prediction tasks\n",
    "normal.linear_regression()\n",
    "normal.support_vector_machine(model_tunning=True)\n",
    "normal.neural_net_mlp(model_tunning=True)\n",
    "# normal.naive_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "03ade08098a545b0940f8a65ad46958e"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "# visualize results\n",
    "normal.vis_results_time_series(second_plot='error')\n"
   ]
  },
  {
   "source": [
    "- even with the volume data which seems more stationary than open price data, the forecasts are still dominated by t-1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e1f8bb26f2614450a8a1a93e6b6953c5"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "# plot predicted vs real value scatter plots\n",
    "normal.vis_results_scatter()"
   ]
  },
  {
   "source": [
    "# 4.0 Play around with standardization and prediction returns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "63270c806ecd4dec81298baf341061b7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           Dates         Open  pct_change  pct_change_cumprod\n",
       "0     2010-06-28  1077.500000    0.000000            1.000000\n",
       "1     2010-06-29  1071.099976   -0.005940            0.994060\n",
       "2     2010-06-30  1040.560059   -0.028513            0.965717\n",
       "3     2010-07-01  1031.099976   -0.009091            0.956937\n",
       "4     2010-07-02  1027.650024   -0.003346            0.953736\n",
       "...          ...          ...         ...                 ...\n",
       "1995  2018-05-31  2720.979980    0.006864            2.525271\n",
       "1996  2018-06-01  2718.699951   -0.000838            2.523155\n",
       "1997  2018-06-04  2741.669922    0.008449            2.544473\n",
       "1998  2018-06-05  2748.459961    0.002477            2.550775\n",
       "1999  2018-06-06  2753.250000    0.001743            2.555220\n",
       "\n",
       "[2000 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dates</th>\n      <th>Open</th>\n      <th>pct_change</th>\n      <th>pct_change_cumprod</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2010-06-28</td>\n      <td>1077.500000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2010-06-29</td>\n      <td>1071.099976</td>\n      <td>-0.005940</td>\n      <td>0.994060</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2010-06-30</td>\n      <td>1040.560059</td>\n      <td>-0.028513</td>\n      <td>0.965717</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2010-07-01</td>\n      <td>1031.099976</td>\n      <td>-0.009091</td>\n      <td>0.956937</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2010-07-02</td>\n      <td>1027.650024</td>\n      <td>-0.003346</td>\n      <td>0.953736</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1995</th>\n      <td>2018-05-31</td>\n      <td>2720.979980</td>\n      <td>0.006864</td>\n      <td>2.525271</td>\n    </tr>\n    <tr>\n      <th>1996</th>\n      <td>2018-06-01</td>\n      <td>2718.699951</td>\n      <td>-0.000838</td>\n      <td>2.523155</td>\n    </tr>\n    <tr>\n      <th>1997</th>\n      <td>2018-06-04</td>\n      <td>2741.669922</td>\n      <td>0.008449</td>\n      <td>2.544473</td>\n    </tr>\n    <tr>\n      <th>1998</th>\n      <td>2018-06-05</td>\n      <td>2748.459961</td>\n      <td>0.002477</td>\n      <td>2.550775</td>\n    </tr>\n    <tr>\n      <th>1999</th>\n      <td>2018-06-06</td>\n      <td>2753.250000</td>\n      <td>0.001743</td>\n      <td>2.555220</td>\n    </tr>\n  </tbody>\n</table>\n<p>2000 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# some misc data\n",
    "x = sp_500['Open'][-2000:]\n",
    "dates = sp_500['Date'][-2000:]\n",
    "# percentage returns\n",
    "x_pct = x.pct_change().fillna(0)\n",
    "x_pct\n",
    "\n",
    "# create new df hold both\n",
    "df = pd.DataFrame(columns=['Dates','Open','pct_change','pct_change_cumprod']) # ,'log_transform'\n",
    "df['Dates'] = dates\n",
    "df['Open'] =  x\n",
    "df['pct_change'] = x_pct\n",
    "df['pct_change_cumprod'] = (x_pct + 1).cumprod()\n",
    "# df['log_transform'] = np.log(df['Open'] )\n",
    "\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "# plot\n",
    "df.plot(subplots=True,sharex=True,figsize=(7,7))\n",
    "plt.tight_layout()\n",
    "\n",
    "# view data\n",
    "df"
   ]
  },
  {
   "source": [
    "- unsure what the log transform is required for"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 4.1 predicting using returns data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ca1020ca5494e0d946c5fcec5960330"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "36e5152857794ba1a677eca98f14d322"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "df['pct_change_normalised'] = scaler.fit_transform(df['pct_change'].to_numpy().reshape(-1, 1))\n",
    "\n",
    "normal = time_series_prediction(df['Dates'],df['pct_change_normalised'],10,1) # pass time series, lag window length, a number of steps ahead to predict\n",
    "normal.sliding_window_1(verbose=0) # time series to supervised learning ML problem\n",
    "normal.train_test_split(split=1500) # testing and training dataset split\n",
    "normal.test_train_plot() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training multivariate linear regression:\n",
      "\n",
      "Linear regression coefficients: \n",
      " [ 0.0399997  -0.03879977 -0.00294204 -0.00418228 -0.00415419 -0.0923927\n",
      " -0.0174214  -0.07352073  0.02916561 -0.01048691]\n",
      "RMSE:  0.060951546047454785\n",
      "MAE:  0.03906715020243121\n",
      "\n",
      "Training support vector machine:\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 14 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  74 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-3)]: Done 152 out of 240 | elapsed:    1.4s remaining:    0.8s\n",
      "[Parallel(n_jobs=-3)]: Done 240 out of 240 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 14 concurrent workers.\n",
      "best_score:  -0.0072276237637182664\n",
      "best_model:  SVR(C=0.1, kernel='sigmoid')\n",
      "best_params:  {'C': 0.1, 'epsilon': 0.1, 'kernel': 'sigmoid'}\n",
      "RMSE:  0.05981790295000705\n",
      "MAE:  0.03859594759057162\n",
      "\n",
      "Training neural network: \n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "[Parallel(n_jobs=-3)]: Done 140 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-3)]: Done 378 out of 405 | elapsed:   30.2s remaining:    2.1s\n",
      "best_score:  -0.007250128767152561\n",
      "best_model:  MLPRegressor(activation='logistic', hidden_layer_sizes=(10,),\n",
      "             learning_rate='invscaling', learning_rate_init=1, max_iter=1000,\n",
      "             shuffle=False)\n",
      "best_params:  {'activation': 'logistic', 'hidden_layer_sizes': (10,), 'learning_rate': 'invscaling', 'learning_rate_init': 1}\n",
      "RMSE:  0.05987884006312983\n",
      "MAE:  0.038784499335365125\n",
      "[Parallel(n_jobs=-3)]: Done 405 out of 405 | elapsed:   48.7s finished\n"
     ]
    }
   ],
   "source": [
    "# perform some prediction tasks\n",
    "normal.linear_regression()\n",
    "normal.support_vector_machine(model_tunning=True)\n",
    "normal.neural_net_mlp(model_tunning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db69f1c3e7c34a0288b8f45ea2f0d871"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "normal.vis_results_time_series(second_plot='error')"
   ]
  },
  {
   "source": [
    "some remarks on predictions using returns of open price:\n",
    "- prediction accuracy of models look terrible. Is it even worth comparing feature engineering approaches if predictions are this bad?\n",
    "- evaluating models using cummulative gains seems reasonable"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 5.0 Denosing using fourier transform "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy fft functions\n",
    "from scipy.fft import fft, ifft, fftfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([7841.0835      -0.j        ,  344.33876287 -95.53936485j,\n",
       "       -101.66935261-291.99051477j, ...,  133.3537504 +170.01192444j,\n",
       "       -101.66935261+291.99051477j,  344.33876287 +95.53936485j])"
      ]
     },
     "metadata": {},
     "execution_count": 168
    }
   ],
   "source": [
    "# apply discrete fourier transform\n",
    "signal = np.array(sp_500['Volume'][-2100:]/1e9) # data\n",
    "fft_coefficients = fft(signal) # fourier transform\n",
    "fft_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "48417c48733241d4955cc213c7695b31"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\tristan\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "# plot orignal signal and inverse fourier transform, shows you can transform signal to frequency domain, then back to time domain\n",
    "inverse_fft = ifft(fft_coefficients)\n",
    "fig,ax = plt.subplots(figsize=(10,4))\n",
    "ax.plot(sp_500['Date'][-2100:],inverse_fft,'-',label='Inverse fourier data')\n",
    "ax.plot(range(0,len(inverse_fft)),signal,'.',label='Real data')\n",
    "ax.set_xlabel('Days')\n",
    "ax.set_xticks([sp_500['Date'][-2100:].iloc[x] for x in range(0,len(sp_500['Date'][-2100:]),120)])\n",
    "ax.legend()\n",
    "ax.tick_params(rotation=30,labelsize=15)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "00f4988caa6442d6b4cea0df03df6e50"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "# plot amplitude vs frequency \n",
    "n = len(signal)\n",
    "\n",
    "# get frequencies and psd\n",
    "freqs = fftfreq(signal.shape[0]) # x axis of amplitude vs frequency graphs\n",
    "psd = np.abs(fft_coefficients)/n # psd is amplitude/N, psd or power spectrum density is the magnitude of the coefficients resulting from fourier transform\n",
    "\n",
    "# plot psd\n",
    "fig,ax = plt.subplots(figsize=(10,5))\n",
    "ax.plot(freqs[1:int(n/2)],psd[1:int(n/2)])\n",
    "ax.set_ylabel('Power spectrum',fontsize=15)\n",
    "ax.set_xlabel('Frequencies',fontsize=15)\n",
    "ax.set_title('FFT')\n",
    "ax.tick_params(labelsize=15)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "source": [
    "observations from coefficient magnitude vs frequency graph:\n",
    "- most frequencies have low amplitude\n",
    "- can denoise signal by setting coefficients with low amplitude to zero - ie a thresholding approach. Here the threshold might be something like 0.06\n",
    "- fyi: frequency = 1 / #days therefore "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "476b743e87da431a931139d5346bacdb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\tristan\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "# Threshold coefficients to denoise signal\n",
    "psd_indices = psd > 0.06 # mask\n",
    "fft_filtered = fft_coefficients*psd_indices\n",
    "\n",
    "# low pass filtering\n",
    "freq_indices = freqs < 0.003 \n",
    "fft_filtered = fft_coefficients*freq_indices\n",
    "\n",
    "# inverse transform filter coefficients\n",
    "inverse_transform_filtered = ifft(fft_filtered)\n",
    "\n",
    "# plot this\n",
    "fig,ax = plt.subplots(figsize=(10,5))\n",
    "ax.plot(sp_500['Date'][-2100:],signal,'-',label='Real data')\n",
    "ax.plot(sp_500['Date'][-2100:],inverse_transform_filtered,'-',label='Inverse fourier filtered')\n",
    "ax.legend()\n",
    "ax.set_title('Threshold = 0.06')\n",
    "ax.set_xlabel('Days',fontsize=15)\n",
    "ax.set_xticks([sp_500['Date'][-2100:].iloc[x] for x in range(0,len(sp_500['Date'][-2100:]),120)])\n",
    "ax.tick_params(rotation=30,labelsize=15)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9c8b1eac747e456dbcd77d949e36ec46"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "# put together artifical data, here the training set is denoised and the testing set is left unchanged\n",
    "df_data = pd.DataFrame(columns=['Dates','artificial_data'])\n",
    "df_data['Dates'] = sp_500['Date'][-2000:]\n",
    "df_data['artificial_data'] = np.concatenate((np.real(inverse_transform_filtered)[-2000:-500],sp_500['Volume'][-500:].to_numpy()/1e9),axis=None)\n",
    "\n",
    "# plot this\n",
    "fig,ax = plt.subplots(figsize=(10,5))\n",
    "ax.plot(sp_500['Date'][-2000:],sp_500['Volume'][-2000:]/1e9,'-',label='Real data')\n",
    "ax.plot(df_data['Dates'],df_data['artificial_data'],'-',label='Artificial testing and training data')\n",
    "ax.legend()\n",
    "ax.set_title('Threshold = 0.06')\n",
    "ax.set_xlabel('Days',fontsize=15)\n",
    "ax.set_xticks([sp_500['Date'][-2000:].iloc[x] for x in range(0,len(sp_500['Date'][-2000:]),120)])\n",
    "ax.tick_params(rotation=30,labelsize=15)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "source": [
    "## 5.1 now run predictions by training on filtered data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# initializing predicition class object for denoise\n",
    "fft_denoised = time_series_prediction(sp_500['Date'][-2000:],df_data['artificial_data'],5,1) # pass time series, lag window length, a number of steps ahead to predict\n",
    "fft_denoised.sliding_window_1(verbose=0) # time series to supervised learning ML problem\n",
    "fft_denoised.train_test_split(split=1500) # testing and training dataset split\n",
    "fft_denoised.test_train_plot() "
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 181,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf8f39d416354c8d875bd82125c71ae4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "90787c283df9464694bd2c23e4096456"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "source": [
    "# perform some prediction tasks\n",
    "fft_denoised.linear_regression()\n",
    "fft_denoised.support_vector_machine(model_tunning=True,C= 100, epsilon= 0.1, kernel= 'linear') # these values come from first training model on normal data\n",
    "fft_denoised.neural_net_mlp(model_tunning=True,activation= 'tanh', hidden_layer_sizes= (1000,), learning_rate= 'constant', learning_rate_init= 0.001) # these values come from first training model on normal data"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 182,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training multivariate linear regression:\n",
      "\n",
      "Linear regression coefficients: \n",
      " [0.12485823 0.04414433 0.06316444 0.17700903 0.47536663]\n",
      "RMSE:  0.5346242481632774\n",
      "MAE:  0.34899960149011056\n",
      "\n",
      "Training support vector machine:\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 14 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  75 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-3)]: Done 152 out of 240 | elapsed:    1.5s remaining:    0.8s\n",
      "[Parallel(n_jobs=-3)]: Done 240 out of 240 | elapsed:    3.3s finished\n",
      "best_score:  -0.08275972620987947\n",
      "best_model:  SVR(C=100, kernel='linear')\n",
      "best_params:  {'C': 100, 'epsilon': 0.1, 'kernel': 'linear'}\n",
      "RMSE:  0.5356419260767656\n",
      "MAE:  0.34698266067530154\n",
      "\n",
      "Training neural network: \n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 14 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done 140 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-3)]: Done 341 tasks      | elapsed:   31.5s\n",
      "[Parallel(n_jobs=-3)]: Done 378 out of 405 | elapsed:   38.5s remaining:    2.7s\n",
      "best_score:  -0.08385066904666261\n",
      "best_model:  MLPRegressor(learning_rate='adaptive', learning_rate_init=0.01, max_iter=1000,\n",
      "             shuffle=False)\n",
      "best_params:  {'activation': 'relu', 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'learning_rate_init': 0.01}\n",
      "RMSE:  0.5455745096042889\n",
      "MAE:  0.3570306086210215\n",
      "[Parallel(n_jobs=-3)]: Done 405 out of 405 | elapsed:   47.1s finished\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "51f19d93352f41d6bfaf11f987e5fd23"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "fft_denoised.vis_results_time_series(second_plot='error')"
   ]
  },
  {
   "source": [
    "## 5.2 Compare denoised results to normal "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f0f6b03f0a424852a06f2b83475ecc73"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dccf23f0f576415ebb9abe6eb677d305"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training multivariate linear regression:\n",
      "\n",
      "Linear regression coefficients: \n",
      " [0.09282714 0.02080688 0.04822414 0.15111368 0.44476971]\n",
      "RMSE:  0.5274919871056635\n",
      "MAE:  0.3471744942971835\n",
      "\n",
      "Training support vector machine:\n",
      "Model params:  {'C': 100, 'cache_size': 200, 'coef0': 0.0, 'degree': 3, 'epsilon': 0.1, 'gamma': 'scale', 'kernel': 'linear', 'max_iter': 1000, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "RMSE:  0.6609905743797734\n",
      "MAE:  0.44956223654614363\n",
      "\n",
      "Training neural network: \n",
      "Model params: {'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'learning_rate_init': 0.01, 'max_fun': 15000, 'max_iter': 1000, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': 1, 'shuffle': False, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "RMSE:  0.5354893830493296\n",
      "MAE:  0.34509062992508055\n",
      "C:\\Users\\tristan\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:246: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n"
     ]
    }
   ],
   "source": [
    "# run predictions on volume data without denoising\n",
    "\n",
    "# initializing predicition class object for denoise\n",
    "normal = time_series_prediction(sp_500['Date'][-2000:],sp_500['Volume'][-2000:]/1e9,5,1) # pass time series, lag window length, a number of steps ahead to predict\n",
    "normal.sliding_window_1(verbose=0) # time series to supervised learning ML problem\n",
    "normal.train_test_split(split=1500) # testing and training dataset split\n",
    "normal.test_train_plot() \n",
    "\n",
    "# perform some prediction tasks\n",
    "normal.linear_regression()\n",
    "normal.support_vector_machine(model_tunning=False,C= 100, epsilon= 0.1, kernel= 'linear')\n",
    "normal.neural_net_mlp(model_tunning=False,activation= 'relu', hidden_layer_sizes= (100,), learning_rate= 'adaptive', learning_rate_init= 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7fbf6abce5ce4c6291018896f7218031"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "# compare results for denoised and normal data: pulling data from predicito objects\n",
    "fig,ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "ax.plot(normal.time_series_dates[normal.training_split+normal.lag_window_length:],normal.y_test,'-',label='real vals')\n",
    "ax.plot(fft_denoised.time_series_dates[fft_denoised.training_split+fft_denoised.lag_window_length:],fft_denoised.linear_reg_predictions,'-',label='linear reg - denoised')\n",
    "ax.plot(fft_denoised.time_series_dates[fft_denoised.training_split+fft_denoised.lag_window_length:],fft_denoised.svm_predictions,'-',label='svm - denoised')\n",
    "ax.plot(fft_denoised.time_series_dates[fft_denoised.training_split+fft_denoised.lag_window_length:],fft_denoised.neural_net_predictions,'-',label='nn - denoised')\n",
    "\n",
    "ax.plot(normal.time_series_dates[normal.training_split+normal.lag_window_length:],normal.linear_reg_predictions,'--',label='linear reg ')\n",
    "ax.plot(normal.time_series_dates[normal.training_split+normal.lag_window_length:],normal.svm_predictions,'--',label='svm ')\n",
    "ax.plot(normal.time_series_dates[normal.training_split+normal.lag_window_length:],normal.neural_net_predictions,'--',label='nn ')\n",
    "\n",
    "ax.set_xticks([normal.time_series_dates[x] for x in range(normal.training_split,len(normal.time_series_dates),28)])\n",
    "ax.tick_params(rotation=30)\n",
    "ax.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "source": [
    "How to compare results before and after denoising?\n",
    "- rmse for predictions of denoised data cant be compared to remse of predictions using normal data because you are comparing against two different signals, one noisy and one denoised."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 6.0 Wavelet denoising"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Drawbacks of fourier transform / denoising:\n",
    "- requires stationary data\n",
    "- no localization of when different frequencies occured\n",
    "- thresholding fourier trasnform coefficients requires setting a hyperparameter - the threshold \n",
    "\n",
    "Benefits of wavelets transform:\n",
    "- data does not need to be stationary\n",
    "- localization of when frequencies occur\n",
    "\n",
    "Drawbacks of wavelets for denoising:\n",
    "- more hyperparameters, threshold value as well as selecting wavelet type"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 6.1 Wavelet transform / decomposition of time series signal"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Wavelet denoisng method:\n",
    "- First perform a wavelet transform of the open data, denoise by thresholding coefficients, then computes returns of denoised signal. Compute returns and perform forecasting. Transform predictions to value and compare. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "maximum level is 8\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d9ed7837dd0d42e0a7dd21869e826305"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "\n",
    "import pywt\n",
    "import sys\n",
    "\n",
    "# Data format:\n",
    "# Raw data should be in a .txt file with two columns, separated by tabs:\n",
    "#  - The first column should be a time-series index\n",
    "#  - The second column should contain the data to be filtered\n",
    "\n",
    "# Time series / data:\n",
    "data = sp_500['Open'][-5000:] \n",
    "\n",
    "index = sp_500['Open'][-5000:].index\n",
    "\n",
    "# Create wavelet object and define parameters\n",
    "w = pywt.Wavelet('sym8') # sym family look good too sym8, this is where you should change the wavelet type, haar wavelet is simply 'haar'\n",
    "maxlev = pywt.dwt_max_level(len(data), w.dec_len)\n",
    "print(\"maximum level is \" + str(maxlev))\n",
    "threshold = 0.8 # Threshold for filtering coefficients as part of denoising, the higher this value the more coefficients you set to zero, ie more of the original signal you truncate away / denoise\n",
    "\n",
    "# Decompose into wavelet components, to the level selected:\n",
    "coeffs = pywt.wavedec(data, w, level=4)\n",
    "\n",
    "# Threshold the wavelet coefficients, thereby removing noise\n",
    "\n",
    "# plt.figure(figsize=(8,15))\n",
    "for i in range(1, len(coeffs)):\n",
    "    # plt.subplot(maxlev, 1, i)\n",
    "    # plt.plot(coeffs[i],label='Original coefficients')\n",
    "    coeffs[i] = pywt.threshold(coeffs[i], threshold*max(coeffs[i]),mode='hard')\n",
    "    # plt.plot(coeffs[i],label='Thresholded coefficients')\n",
    "    # plt.ylabel('Scale: '+str(maxlev-i+1))\n",
    "    # plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "# inverse transform coefficient to reconstruct time series signal, minus noise\n",
    "datarec = pywt.waverec(coeffs, w)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(index, data,label='Raw signal')\n",
    "plt.plot(index, datarec,label=\"De-noised signal using wavelet techniques\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distance measures between true signal and denoised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0  :  34\n",
      "1  :  34\n",
      "2  :  53\n",
      "3  :  92\n",
      "4  :  170\n",
      "5  :  326\n",
      "6  :  638\n",
      "7  :  1261\n",
      "8  :  2507\n",
      "<ipython-input-48-b8260bc4145e>:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  coeffs_array = np.array(coeffs)\n"
     ]
    }
   ],
   "source": [
    "coeffs = pywt.wavedec(data, w, level=maxlev)\n",
    "coeffs_array = np.array(coeffs)\n",
    "for i in range(len(coeffs)):\n",
    "    print(i,' : ',len(coeffs[i]))"
   ]
  },
  {
   "source": [
    "## 6.2 Predictions: normal data vs wavelet denoised"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "predict sp500 open price one day ahead, using precentage returns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some data processing: \n",
    "\n",
    "# create new df for normal data\n",
    "df_normal = pd.DataFrame(columns=['Dates','Open','pct_change','pct_change_cumprod'])\n",
    "df_normal['Dates'] = sp_500['Date'][-5000:]\n",
    "df_normal['Open'] =   sp_500['Open'][-5000:]\n",
    "df_normal['pct_change'] = df_normal['Open'].pct_change().fillna(0)\n",
    "df_normal['pct_change_cumprod'] = (df_normal['pct_change']  + 1).cumprod()\n",
    "\n",
    "df_normal.reset_index(inplace=True,drop=True)\n",
    "\n",
    "# create new df for wavelet denoised data\n",
    "df_denoised= pd.DataFrame(columns=['Dates','Open','pct_change','pct_change_cumprod'])\n",
    "df_denoised['Dates'] = sp_500['Date'][-5000:]\n",
    "df_denoised['Open'] =   datarec\n",
    "df_denoised['pct_change'] = df_denoised['Open'].pct_change().fillna(0)\n",
    "df_denoised['pct_change_cumprod'] = (df_denoised['pct_change']  + 1).cumprod()\n",
    "\n",
    "df_denoised.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "34fed73c60af4e51b2da2e1ced115990"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\tristan\\anaconda3\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\__init__.py:61: UserWarning: To output multiple subplots, the figure containing the passed axes is being cleared\n  plot_obj.generate()\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a3b81e8b70384a45a072d60a39ba146d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\tristan\\anaconda3\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\__init__.py:61: UserWarning: To output multiple subplots, the figure containing the passed axes is being cleared\n  plot_obj.generate()\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([<AxesSubplot:>, <AxesSubplot:>, <AxesSubplot:>], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,7))\n",
    "df_normal.plot(subplots=True,ax=ax)\n",
    "fig,ax = plt.subplots(figsize=(10,7))\n",
    "df_denoised.plot(subplots=True,ax=ax)"
   ]
  },
  {
   "source": [
    "## 6.2 Perform prediction on data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a12e783307f145769773d7c0f4f873f5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "493a76545daa47a5aa6c472e2d4837d7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training multivariate linear regression:\n",
      "\n",
      "Linear regression coefficients: \n",
      " [ 0.00827728 -0.01519066  0.01269019 -0.03919226 -0.00528929 -0.05123576\n",
      " -0.011838    0.00053218 -0.05485617 -0.0432033 ]\n",
      "RMSE:  0.006622549754153012\n",
      "MAE:  0.004311865851622058\n",
      "\n",
      "Training support vector machine:\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 14 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  84 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-3)]: Done 213 out of 240 | elapsed:    1.6s remaining:    0.1s\n",
      "[Parallel(n_jobs=-3)]: Done 240 out of 240 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 14 concurrent workers.\n",
      "best_score:  -0.0002024966760961271\n",
      "best_model:  SVR(C=0.1, epsilon=100, kernel='linear')\n",
      "best_params:  {'C': 0.1, 'epsilon': 100, 'kernel': 'linear'}\n",
      "RMSE:  0.011372765579198503\n",
      "MAE:  0.009777494110736069\n",
      "\n",
      "Training neural network: \n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "[Parallel(n_jobs=-3)]: Done 140 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=-3)]: Done 335 tasks      | elapsed:   47.3s\n",
      "[Parallel(n_jobs=-3)]: Done 378 out of 405 | elapsed:  1.0min remaining:    4.3s\n",
      "best_score:  -0.00014193589817502336\n",
      "best_model:  MLPRegressor(hidden_layer_sizes=(10,), learning_rate='invscaling',\n",
      "             learning_rate_init=0.01, max_iter=1000, shuffle=False)\n",
      "best_params:  {'activation': 'relu', 'hidden_layer_sizes': (10,), 'learning_rate': 'invscaling', 'learning_rate_init': 0.01}\n",
      "RMSE:  0.006616881896142459\n",
      "MAE:  0.004300205007511253\n",
      "[Parallel(n_jobs=-3)]: Done 405 out of 405 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "add1d42333be45bd8ba7fb70c634111e"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "########################################################################\n",
    "# forecasting on normal data\n",
    "########################################################################\n",
    "\n",
    "normal = time_series_prediction(df_normal['Dates'],df_normal['pct_change'],10,1) # pass time series, lag window length, a number of steps ahead to predict\n",
    "normal.sliding_window_1(verbose=0) # time series to supervised learning ML problem\n",
    "normal.train_test_split(split=4500) # testing and training dataset split\n",
    "normal.test_train_plot()    # visualize training split\n",
    "\n",
    "# perform some prediction tasks\n",
    "normal.linear_regression()\n",
    "normal.support_vector_machine(model_tunning=True)\n",
    "normal.neural_net_mlp(model_tunning=True)\n",
    "\n",
    "#visualize results\n",
    "normal.vis_results_time_series(second_plot='cumprod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "429af1d638ef43d7bdc31669f5814374"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1278805bcb0f4990a081061f1a8fcbd0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training multivariate linear regression:\n",
      "\n",
      "Linear regression coefficients: \n",
      " [ 0.0237145  -0.02246668  0.04740009  0.00701641  0.03584964 -0.03737366\n",
      " -0.03726144  0.05776496  0.03446961  0.17203609]\n",
      "RMSE:  0.004456301705215824\n",
      "MAE:  0.0016611758073728459\n",
      "\n",
      "Training support vector machine:\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 14 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  70 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-3)]: Done 240 out of 240 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 14 concurrent workers.\n",
      "best_score:  -0.00023433945042440927\n",
      "best_model:  SVR(C=0.1, epsilon=10, kernel='linear')\n",
      "best_params:  {'C': 0.1, 'epsilon': 10, 'kernel': 'linear'}\n",
      "RMSE:  0.015381430721325244\n",
      "MAE:  0.014829075969529875\n",
      "\n",
      "Training neural network: \n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "[Parallel(n_jobs=-3)]: Done 140 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-3)]: Done 329 tasks      | elapsed:   53.2s\n",
      "[Parallel(n_jobs=-3)]: Done 378 out of 405 | elapsed:  1.2min remaining:    5.1s\n",
      "best_score:  -3.784801693916872e-05\n",
      "best_model:  MLPRegressor(activation='logistic', hidden_layer_sizes=(10,),\n",
      "             learning_rate='invscaling', learning_rate_init=0.01, max_iter=1000,\n",
      "             shuffle=False)\n",
      "best_params:  {'activation': 'logistic', 'hidden_layer_sizes': (10,), 'learning_rate': 'invscaling', 'learning_rate_init': 0.01}\n",
      "RMSE:  0.0044628414884044796\n",
      "MAE:  0.0017067176124213601\n",
      "[Parallel(n_jobs=-3)]: Done 405 out of 405 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d5c199b9c124efd828cabb1fba45f18"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "########################################################################\n",
    "# forecasting on denoised data\n",
    "########################################################################\n",
    "\n",
    "denoised = time_series_prediction(df_denoised['Dates'],df_denoised['pct_change'],10,1) # pass time series, lag window length, a number of steps ahead to predict\n",
    "denoised.sliding_window_1(verbose=0) # time series to supervised learning ML problem\n",
    "denoised.train_test_split(split=4500) # testing and training dataset split\n",
    "denoised.test_train_plot() \n",
    "\n",
    "# perform some prediction tasks\n",
    "denoised.linear_regression()\n",
    "denoised.support_vector_machine(model_tunning=True,C= 0.1, epsilon= 10, kernel= 'linear')\n",
    "denoised.neural_net_mlp(model_tunning=True,activation= 'relu', hidden_layer_sizes= (100,), learning_rate= 'adaptive', learning_rate_init= 0.001)\n",
    "\n",
    "#visualize results\n",
    "denoised.vis_results_time_series(second_plot='cumprod')"
   ]
  },
  {
   "source": [
    "## 6.3 Compare results for normal vs denoised"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8cfc121117ce4d74b2ee60f7e465639f"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "# compare results for denoised and normal data\n",
    "fig,ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "ax.plot(normal.time_series_dates[normal.training_split+normal.lag_window_length:],normal.real_vals_cumprod,'-',label='real vals cumprod',linewidth=3)\n",
    "ax.plot(denoised.time_series_dates[denoised.training_split+denoised.lag_window_length:],denoised.linear_reg_predictions_cumprod,'-',label='linear reg cumprod - denoised')\n",
    "# ax.plot(denoised.time_series_dates[denoised.training_split+denoised.lag_window_length:],denoised.svm_predictions_cumprod,'-',label='svm cumprod - denoised')\n",
    "ax.plot(denoised.time_series_dates[denoised.training_split+denoised.lag_window_length:],denoised.neural_net_predictions_cumprod,'-',label='nn cumprod - denoised')\n",
    "\n",
    "ax.plot(normal.time_series_dates[normal.training_split+normal.lag_window_length:],normal.linear_reg_predictions_cumprod,'--',label='linear reg cumprod')\n",
    "# ax.plot(normal.time_series_dates[normal.training_split+normal.lag_window_length:],normal.svm_predictions_cumprod,'--',label='svm cumprod')\n",
    "ax.plot(normal.time_series_dates[normal.training_split+normal.lag_window_length:],normal.neural_net_predictions_cumprod,'--',label='nn cumprod')\n",
    "\n",
    "ax.set_xticks([normal.time_series_dates[x] for x in range(normal.training_split,len(normal.time_series_dates),28)])\n",
    "ax.tick_params(rotation=30)\n",
    "ax.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "source": [
    "## 6.4 transform return predictions back to price data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "When testing denoising methods, we need to compare against predictions without denoising. But once you denoise the original signal, you cant compare the RMSE metric of the denoised results to that of the normal (without denoising) prediction method because these metrics are computed against different based y_true values. So: \n",
    "\n",
    "\n",
    "- 1) transform price to returns\n",
    "- 2) predict returns with and without denoising\n",
    "- 3) convert returns to price and compute rmse, with and without denosing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-115-c730201f51f1>:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_normal_results['linear_reg_prices'] = df_normal['Open'][4521] * normal.linear_reg_predictions_cumprod\n<ipython-input-115-c730201f51f1>:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_normal_results['svm_reg_prices'] = df_normal['Open'][4521] * normal.svm_predictions_cumprod\n<ipython-input-115-c730201f51f1>:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_normal_results['nn_reg_prices'] = df_normal['Open'][4521] * normal.neural_net_predictions_cumprod\n<ipython-input-115-c730201f51f1>:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_denoised_results['linear_reg_prices'] = df_denoised['Open'][4521] * denoised.linear_reg_predictions_cumprod\n<ipython-input-115-c730201f51f1>:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_denoised_results['svm_reg_prices'] = df_denoised['Open'][4521] * denoised.svm_predictions_cumprod\n<ipython-input-115-c730201f51f1>:13: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_denoised_results['nn_reg_prices'] = df_denoised['Open'][4521] * denoised.neural_net_predictions_cumprod\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "516da746127744cfa661cad887876b16"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "# define some new dataframes to hold all data\n",
    "df_normal_results = df_normal.iloc[4521:,:]\n",
    "df_denoised_results = df_denoised.iloc[4521:,:]\n",
    "\n",
    "# no denoising\n",
    "df_normal_results['linear_reg_prices'] = df_normal['Open'][4521] * normal.linear_reg_predictions_cumprod\n",
    "df_normal_results['svm_reg_prices'] = df_normal['Open'][4521] * normal.svm_predictions_cumprod\n",
    "df_normal_results['nn_reg_prices'] = df_normal['Open'][4521] * normal.neural_net_predictions_cumprod\n",
    "\n",
    "# with denoising\n",
    "df_denoised_results['linear_reg_prices'] = df_denoised['Open'][4521] * denoised.linear_reg_predictions_cumprod\n",
    "df_denoised_results['svm_reg_prices'] = df_denoised['Open'][4521] * denoised.svm_predictions_cumprod\n",
    "df_denoised_results['nn_reg_prices'] = df_denoised['Open'][4521] * denoised.neural_net_predictions_cumprod\n",
    "\n",
    "# plot results\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(df_normal_results['Dates'],df_normal_results['Open'],label='Real open data')\n",
    "\n",
    "plt.plot(df_normal_results['Dates'],df_normal_results['linear_reg_prices'],label='linear normal')\n",
    "# plt.plot(df_normal_results['Dates'],df_normal_results['svm_reg_prices'],label='svm normal')\n",
    "plt.plot(df_normal_results['Dates'],df_normal_results['nn_reg_prices'],label='nn normal')\n",
    "\n",
    "plt.plot(df_normal_results['Dates'],df_denoised_results['linear_reg_prices'],'--',label='linear denoised')\n",
    "# plt.plot(df_normal_results['Dates'],df_denoised_results['svm_reg_prices'],label='svm denoised')\n",
    "plt.plot(df_normal_results['Dates'],df_denoised_results['nn_reg_prices'],'--',label='nn denoised')\n",
    "\n",
    "\n",
    "plt.xticks([df_normal_results['Dates'].iloc[x] for x in range(0,len(df_normal_results['Dates'][:]),28)])\n",
    "plt.tick_params(rotation=30)\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Linear normal - RMSE cumulatic gains:\t 273.7957156081383\nSVM normal - RMSE cumulatic gains:\t 73853.8022660274\nANN normal - RMSE cumulatic gains:\t 166.67343763907598\nLinear denoised - RMSE cumulatic gains:\t 178.58542289582076\nSVM denoised - RMSE cumulatic gains:\t 809173.9539076653\nANN denoised - RMSE cumulatic gains:\t 306.7646744121744\n"
     ]
    }
   ],
   "source": [
    "# compute evaluation metrics: look at RMSE between cummulative gains of real data vs predictions with and without denoising\n",
    "\n",
    "# data\n",
    "y_true = df_normal_results['Open']\n",
    "\n",
    "# no denoising\n",
    "y_pred_1 = df_normal_results['linear_reg_prices']\n",
    "y_pred_2 = df_normal_results['svm_reg_prices']\n",
    "y_pred_3 = df_normal_results['nn_reg_prices']\n",
    "\n",
    "# with denoising\n",
    "y_pred_4 = df_denoised_results['linear_reg_prices']\n",
    "y_pred_5 = df_denoised_results['svm_reg_prices']\n",
    "y_pred_6 = df_denoised_results['nn_reg_prices']\n",
    "\n",
    "# metrics\n",
    "\n",
    "rmse_linear_normal = mean_squared_error(y_true,y_pred_1)\n",
    "rmse_svm_normal = mean_squared_error(y_true,y_pred_2)\n",
    "rmse_ann_normal = mean_squared_error(y_true,y_pred_3)\n",
    "\n",
    "rmse_linear_denoised = mean_squared_error(y_true,y_pred_4)\n",
    "rmse_svn_denoised = mean_squared_error(y_true,y_pred_5)\n",
    "rmse_ann_denoised = mean_squared_error(y_true,y_pred_6)\n",
    "\n",
    "# print metrics\n",
    "print('Linear normal - RMSE cumulatic gains:\\t',rmse_linear_normal**0.5)\n",
    "print('SVM normal - RMSE cumulatic gains:\\t',rmse_svm_normal**0.5)\n",
    "print('ANN normal - RMSE cumulatic gains:\\t',rmse_ann_normal**0.5)\n",
    "\n",
    "print('Linear denoised - RMSE cumulatic gains:\\t',rmse_linear_denoised**0.5)\n",
    "print('SVM denoised - RMSE cumulatic gains:\\t',rmse_svn_denoised**0.5)\n",
    "print('ANN denoised - RMSE cumulatic gains:\\t',rmse_ann_denoised**0.5)\n",
    "\n"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Linear normal - RMSE cumulatic gains:\t 103.78796952459727\n",
    "SVM normal - RMSE cumulatic gains:\t 2119.0777666368795\n",
    "ANN normal - RMSE cumulatic gains:\t 229.97477972358672\n",
    "Linear denoised - RMSE cumulatic gains:\t 49.60068080046763\n",
    "SVM denoised - RMSE cumulatic gains:\t 14452.351946378596\n",
    "ANN denoised - RMSE cumulatic gains:\t 115.447881267899"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Summary of takeaways from denoising using signal processing techniques:\n",
    "- Question, should you denoise before are after computing returns? \n",
    "    - Fourier transform needs to be computed on stationary signal(the sinusoids continue through infinity ie stationary), therefore you must do returns first?\n",
    "    - Wavelet transform can be computed for non-stationary signals - see nice denosing of s&p 500 open prices\n",
    "\n",
    "- How do we compare forecasting results for different denosing results?\n",
    "    - RMSE or MAE against the denoised signals means we are comparing the forecasting results of fourier and wavelet denoised against different signals?\n",
    "    - If we look at cumulative returns over testing dataset, then do we compare against the original cummulative returns?\n",
    "\n",
    "- Some hyperparameters for wavelet transform:\n",
    "    - type of wavelet, should be chosen based on data, all papers I've read have used the haar wavelet. Sym look better in my results.\n",
    "    - Once the dwt transform is applied then a thresholding approach can be applied to set low coefficients to zero. Then iDWT taken to retrieve denoised signal. This threshold value and type of thresholding are another hyperparameter. \n",
    "    - The level of decomposition is also a hyperparameter. \n",
    "\n",
    "- Some hyperparameters for fourier transform:\n",
    "    - thresholding value of different frequencies."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# some random extras"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "401b5912bb684288aab8a1991e0e2e3e"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "import pywt\n",
    "\n",
    "# single level wavelet denoising\n",
    "data = sp_500['Volume'][-2000:]/1e9\n",
    "plt.figure(figsize=(15,5))\n",
    "data.plot()\n",
    "\n",
    "x = np.array(data)                \n",
    "(ca, cd) = pywt.dwt(x, \"sym20\")                \n",
    "cat = pywt.threshold(ca, 0.5, mode=\"hard\")                \n",
    "cdt = pywt.threshold(cd, 0.5, mode=\"soft\")                \n",
    "tx = pywt.idwt(cat, cdt, \"sym20\")\n",
    "\n",
    "plt.plot(sp_500['Volume'][-2000:].index,tx,'-.')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['bior1.1',\n",
       " 'bior1.3',\n",
       " 'bior1.5',\n",
       " 'bior2.2',\n",
       " 'bior2.4',\n",
       " 'bior2.6',\n",
       " 'bior2.8',\n",
       " 'bior3.1',\n",
       " 'bior3.3',\n",
       " 'bior3.5',\n",
       " 'bior3.7',\n",
       " 'bior3.9',\n",
       " 'bior4.4',\n",
       " 'bior5.5',\n",
       " 'bior6.8',\n",
       " 'coif1',\n",
       " 'coif2',\n",
       " 'coif3',\n",
       " 'coif4',\n",
       " 'coif5',\n",
       " 'coif6',\n",
       " 'coif7',\n",
       " 'coif8',\n",
       " 'coif9',\n",
       " 'coif10',\n",
       " 'coif11',\n",
       " 'coif12',\n",
       " 'coif13',\n",
       " 'coif14',\n",
       " 'coif15',\n",
       " 'coif16',\n",
       " 'coif17',\n",
       " 'db1',\n",
       " 'db2',\n",
       " 'db3',\n",
       " 'db4',\n",
       " 'db5',\n",
       " 'db6',\n",
       " 'db7',\n",
       " 'db8',\n",
       " 'db9',\n",
       " 'db10',\n",
       " 'db11',\n",
       " 'db12',\n",
       " 'db13',\n",
       " 'db14',\n",
       " 'db15',\n",
       " 'db16',\n",
       " 'db17',\n",
       " 'db18',\n",
       " 'db19',\n",
       " 'db20',\n",
       " 'db21',\n",
       " 'db22',\n",
       " 'db23',\n",
       " 'db24',\n",
       " 'db25',\n",
       " 'db26',\n",
       " 'db27',\n",
       " 'db28',\n",
       " 'db29',\n",
       " 'db30',\n",
       " 'db31',\n",
       " 'db32',\n",
       " 'db33',\n",
       " 'db34',\n",
       " 'db35',\n",
       " 'db36',\n",
       " 'db37',\n",
       " 'db38',\n",
       " 'dmey',\n",
       " 'haar',\n",
       " 'rbio1.1',\n",
       " 'rbio1.3',\n",
       " 'rbio1.5',\n",
       " 'rbio2.2',\n",
       " 'rbio2.4',\n",
       " 'rbio2.6',\n",
       " 'rbio2.8',\n",
       " 'rbio3.1',\n",
       " 'rbio3.3',\n",
       " 'rbio3.5',\n",
       " 'rbio3.7',\n",
       " 'rbio3.9',\n",
       " 'rbio4.4',\n",
       " 'rbio5.5',\n",
       " 'rbio6.8',\n",
       " 'sym2',\n",
       " 'sym3',\n",
       " 'sym4',\n",
       " 'sym5',\n",
       " 'sym6',\n",
       " 'sym7',\n",
       " 'sym8',\n",
       " 'sym9',\n",
       " 'sym10',\n",
       " 'sym11',\n",
       " 'sym12',\n",
       " 'sym13',\n",
       " 'sym14',\n",
       " 'sym15',\n",
       " 'sym16',\n",
       " 'sym17',\n",
       " 'sym18',\n",
       " 'sym19',\n",
       " 'sym20']"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "pywt.wavelist(kind='discrete')"
   ]
  }
 ]
}